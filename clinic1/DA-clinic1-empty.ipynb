{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7e3a9aff",
   "metadata": {},
   "source": [
    "Data Analysis: Clinic 1 - Group 15\n",
    "\n",
    "Names: Emma Calvino, Ingrid Salvador, Miriam Espinosa\n",
    "\n",
    "Numbers: i6328339, i6314966"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d5ab070",
   "metadata": {},
   "source": [
    "**Use of genAI tools (e.g. chatGPT), websites (e.g. stackoverflow)**: *list websites where you found code (or other info) as well as include information on how you used genAI tools*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a108445",
   "metadata": {},
   "source": [
    "# Data Analysis, Clinic 1\n",
    "# FIETS: Fundamentele Innovatie En Technologie in Scholing\n",
    "## Met FIETS blijft het onderwijs vooruitgaan, zelfs tegen de wind in!\n",
    "\n",
    "---\n",
    "\n",
    "By completing and delivering the clinic tasks you will know how to :\n",
    "\n",
    "- Load data and handle data using pandas;\n",
    "- Navigate the documentation of Python packages by yourself;\n",
    "- Filter and tidy up **noisy** real-world datasets;\n",
    "- Aggregate your data in different (and hopefully helpful) ways;\n",
    "- Use EDA to learn more about your data\n",
    "- Create and interpret informative visualizations to explore the data set\n",
    "- Derive meaningful insights for the societal impact of datasets\n",
    "\n",
    "---\n",
    "**Important Dates.**\n",
    "\n",
    "- Clinic 1 release: Thu 30 Jan 2024\n",
    "- Clinic 1 due: Fri 07 Feb 2024 late night, wildcards available\n",
    "\n",
    "**Instructions for the deliverable:**\n",
    "\n",
    "* You are allowed to use any built-in Python library that comes with Anaconda. If you want to use an external library, you may do so, but must justify your choice.\n",
    "\n",
    "* Make sure that you include a proper amount/mix of comments, results and code. More specifically, be sure to provide a concise textual description of your thought process, the assumptions you made, the solution you implemented, and explanations for your answers. A notebook that only has code cells will not suffice. To avoid confusion: use short comments for longer code answers.\n",
    "\n",
    "* For questions containing the /Discuss:/ prefix, answer not with code, but with a textual explanation (in markdown).\n",
    "\n",
    "* Back up any hypotheses and claims with data, since this is an important aspect of the course.\n",
    "\n",
    "* Please write all your comments in English, and use meaningful variable names (as possible) in your code. \n",
    "\n",
    "* In the end, make sure that all cells are executed properly and everything you need to show is in your (execucted) notebook. We will not run your notebook for you! \n",
    "\n",
    "- In continuation to the previous point, interactive plots, such as those generated using the ‚Äòplotly‚Äô package, should be strictly avoided! Make sure to print results and/or dataframes that confirm you have properly addressed the task.\n",
    "\n",
    "* You are asked to deliver **only your executed notebook file, .ipnyb** and nothing else. If you deliver other files, we will not grade anything.\n",
    "\n",
    "* Honor code applies to these tasks. If you are not certain about an action, consult with Jerry.\n",
    "\n",
    "**A Note from Jerry on using Language Models (LMs)**\n",
    "\n",
    "If you try hard enough, you will likely get away with cheating (that does not only apply to LMs). Fortunately, my job is not to police, but rather to educate you. So, please consider the following:\n",
    "\n",
    "I assume that you are taking this course to learn something! LMs are not always right ([they often fail in silly ways](https://community.openai.com/t/why-9-11-is-larger-than-9-9-incredible/869824/4)). This course should prepare you to detect when they are wrong!\n",
    "\n",
    "I don't restrict the use of LMs because I see the value of being helped when coding (esp. in the context of pandas dataframes nightmare :)). Based on what we saw last year in your notebooks, it's pretty clear when you \"copy\" some code and then you struggle to interpret the results. This is the essence of this course and of the skills you should try build for yourself: Many people can run fancy models these days but not many people can interpret the results correctly. Try to be the latter ones.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9b54879",
   "metadata": {},
   "source": [
    "## Context\n",
    "\n",
    "AI is booming! Newspapers, influencers and your relatives all agree that AI is important. But while almost everyone agrees that AI is the future, much is unclear about what that future esp. in critical sectors like education looks like...\n",
    "\n",
    "Freshly graduated from a top Dutch university in Limburg, you are hired by the Dutch government to advise on a large-scale ‚Äúeducation innovation‚Äù initiative code-named \"FIETS\" (Flexibele Innovatie voor Effici√´nte Toepassing in Scholing). With higher education facing severe budget cuts, the government is looking for creative solutions to \"do more with less.\" Convinced by the stunning progress in language modeling, officials believe LLMs could help battle growing teacher shortages and reduce costs by automating parts of the education process. Your job description: investigate which LMs might be best suited to plug the gaps without draining the budget!\n",
    "\n",
    "You are handed the results of three LMs on the [‚ÄúMassive Multitask Language Understanding (MMLU)‚Äù](https://arxiv.org/abs/2009.03300) dataset  to compare. This famous dataset consists of 57 subjects with multiple-choice questions, covering diverse subjects like mathematics, computer science, history, and law. Most providers of state-of-the-art LMs use this dataset to showcase the versatility of their latest models. Unfortunately, the intern responsible for collecting the results, didn‚Äôt pay attention during DACS KEN3450: Data Analysis. As a result, the collected datasets are slightly corrupted. Jammer!\n",
    "\n",
    "The success of FIETS depends on your ability to make sense of the messy data and recommend the best model to keep the Dutch education system pedaling forward‚Äîdespite uphill challenges like funding shortages and a skeptical academic community!\n",
    "\n",
    "### A very brief primer on Language Models\n",
    "We studied LLMs in the context of the NLP course but here is a short reminder. Language models (LMs) are sophisticated statistical models designed to understand and generate human-like text. At their core, LMs are trained to predict the most likely continuation of a given input text. For example, given the input \"The cat sat on the,\" an LM might predict \"mat\" as a likely continuation.\n",
    "LMs are trained on vast text samples from various sources, including books, websites, and social media. This extensive training allows them to capture patterns and relationships in language, enabling them to generate coherent and contextually appropriate text across a wide range of topics and styles.\n",
    "\n",
    "While LMs can produce text that appears to be written by intelligent humans, it's important to note that their capabilities can diverge from human intelligence in unexpected ways. They may sometimes generate factually incorrect information or struggle with complex reasoning tasks.\n",
    "\n",
    "Two key concepts in understanding LMs are:\n",
    "1. **Tokens**: LMs process text using \"tokens\" rather than individual characters. Tokens can be words, parts of words, or punctuation marks. For example, the sentence \"I love AI!\" might be tokenized as [\"I\", \"love\", \"AI\", \"!\"]. Tokenization is the first step in both training and using an LM.\n",
    "2. **Context**: The input text provided to an LM is called the \"context.\" This context informs the model's predictions or generations. A longer or more specific context often leads to more accurate and relevant outputs.\n",
    "\n",
    "[See: Wikipedia entry on language models](https://en.wikipedia.org/wiki/Large_language_model)\n",
    "\n",
    "###  Files for this assignment\n",
    "This assignment is divided into three tasks, each of which should bring you a step closer to providing a recommendation toward project the objectives of FIETS:\n",
    "\n",
    "- **Task 1**: Inspecting the results and getting your first model ranking\n",
    "- **Task 2**: Inspecting the underlying data used to generate the results for possible biases\n",
    "- **Task 3**: Learning about tokens and providing a final recommendation\n",
    "\n",
    "\n",
    "```\n",
    "üìÅ FIETS\n",
    "‚îÇ\n",
    "‚îú‚îÄ‚îÄ üìÑ clinic1.ipynb (the file you're currently reading!)\n",
    "‚îÇ\n",
    "‚îî‚îÄ‚îÄ üìÅ data\n",
    "    ‚îú‚îÄ‚îÄ üìÅ task_1\n",
    "    ‚îú‚îÄ‚îÄ üìÅ task_2\n",
    "    ‚îî‚îÄ‚îÄ üìÅ task_2.5\n",
    "```   \n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1c8fd700",
   "metadata": {},
   "outputs": [],
   "source": [
    "# some basic imports\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "from scipy.stats import ttest_ind"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd2a7be4",
   "metadata": {},
   "source": [
    "## Task 1 (18 points): What's in an average anyway?\n",
    "\n",
    "The files needed to complete task 1 can be found in the folder \"`data/task_1/`:\n",
    "```\n",
    "task_1/\n",
    "‚îÇ\n",
    "‚îú‚îÄ‚îÄ mmlu_data/\n",
    "‚îÇ   ‚îî‚îÄ‚îÄ test.csv\n",
    "‚îÇ\n",
    "‚îî‚îÄ‚îÄ lm_scores/\n",
    "    ‚îú‚îÄ‚îÄ lm_X.csv\n",
    "    ‚îú‚îÄ‚îÄ lm_Y.csv\n",
    "    ‚îî‚îÄ‚îÄ lm_Z.csv\n",
    "```\n",
    "\n",
    "We will start by loading, (manually) inspecting, and cleaning the data. Although it doesn't seem \"glamorous\" (nor is it particularly fun...) - manually inspecting data is extremely important! In fact, it's one of the few things most AI and Data Science researchers agree on :). Next, we will take a first pass on ordering our Olympic podium between three LMs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16518de6",
   "metadata": {},
   "source": [
    "### 1.1 (1 pt)\n",
    " \n",
    "Load the subfiles contained in the `mmlu_data` and `lm_scores` folders into separate dataframes:\n",
    "- `df_test`\n",
    "- `df_x`\n",
    "- `df_y`\n",
    "- `df_z`\n",
    "\n",
    "for each, print their sizes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "80eabdcb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df_test:  (14042, 8)\n",
      "df_x:  (13882, 2)\n",
      "df_y:  (13978, 2)\n",
      "df_z:  (13923, 2)\n"
     ]
    }
   ],
   "source": [
    "df_test = pd.read_csv('data/task_1/mmlu_data/test.csv')\n",
    "\n",
    "f = 'data/task_1/lm_scores/'\n",
    "df_x = pd.read_csv(os.path.join(f, 'lm_X.csv'))\n",
    "df_y = pd.read_csv(os.path.join(f, 'lm_Y.csv'))\n",
    "df_z = pd.read_csv(os.path.join(f, 'lm_Z.csv'))\n",
    "\n",
    "print('df_test: ', df_test.shape)\n",
    "print('df_x: ', df_x.shape)\n",
    "print('df_y: ', df_y.shape)\n",
    "print('df_z: ', df_z.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da14c5e2",
   "metadata": {},
   "source": [
    "### 1.2 (4 pt)\n",
    "Unfortunately, LMs don't always output the format we want. In the column `result`, the value should be one of A, B, C, or D. \n",
    "\n",
    "A. For each of the LM score dataframes, use a `value_counts()` operation and print the results. \n",
    "\n",
    "B. /Discuss:/ Inspect the results and describe the types of answer formats you see. Besides the \"expected\" case, you should be able to find at least four unexpected formats."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "eeebdb42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df_test:  question                                                                                                                                                                                                                                                                                                                                                      A                                                   B                                                  C                                                        D                                                                 answer  subject                 question_id\n",
      " A 10% increase (decrease) in price produces a 10% decrease (increase) in quantity demanded. This is referred to as:                                                                                                                                                                                                                                          Zero price elasticity of demand.                    Infinite price elasticity of demand.               Unit price elasticity of demand.                         Indefinite price elasticity of demand.                            C       marketing               7162           1\n",
      "The pulse can be palpated:                                                                                                                                                                                                                                                                                                                                    every time the atria contracts.                     when a vein is close to the surface of the skin.   every time the left ventricle contracts.                 when an artery is close to the surface of the skin.               D       clinical knowledge      619            1\n",
      "The process of identifying publics who are involved and affected by a situation central to an organization is called a(n)                                                                                                                                                                                                                                     exploratory survey                                  situation interview                                communication audit                                      stakeholder analysis                                              D       public relations        13074          1\n",
      "The process of invasion and succession is a process involving migration and is best described as when                                                                                                                                                                                                                                                         new residents tear down and rebuild housing units.  new residents live peacefully with old residents.  new residents assimilate with old residents completely.  new residents move into areas occupied by older resident groups.  D       high school geography   3505           1\n",
      "The process of keeping a running account of merchandise on hand is referred to as                                                                                                                                                                                                                                                                             perpetual inventory                                 economic order quantity                            book inventory                                           physical inventory                                                A       marketing               7236           1\n",
      "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              ..\n",
      "For which of these two scenarios does the main character (who uses I/me/my) do something clearly morally wrong, according to ordinary moral standards in the US as of 2020? Scenario 1 | As my roommate slept, I deleted his essay. Scenario 2 | While driving down the road, I ran over a speed bump.                                                        Wrong, Wrong                                        Wrong, Not wrong                                   Not wrong, Wrong                                         Not wrong, Not wrong                                              B       moral scenarios         9392           1\n",
      "For which of these two scenarios does the main character (who uses I/me/my) do something clearly morally wrong, according to ordinary moral standards in the US as of 2020? Scenario 1 | As the baby laid asleep, I cooked them. Scenario 2 | I had five dollars to buy a present for my mom, so I only spent five cents on candy for myself.                 Wrong, Wrong                                        Wrong, Not wrong                                   Not wrong, Wrong                                         Not wrong, Not wrong                                              B       moral scenarios         8607           1\n",
      "For which of these two scenarios does the main character (who uses I/me/my) do something clearly morally wrong, according to ordinary moral standards in the US as of 2020? Scenario 1 | As the baby laid asleep, I smiled at them.  Scenario 2 | I found my sisters fashion magazine by her bed and read it.                                                 Wrong, Wrong                                        Wrong, Not wrong                                   Not wrong, Wrong                                         Not wrong, Not wrong                                              D       moral scenarios         8622           1\n",
      "For which of these two scenarios does the main character (who uses I/me/my) do something clearly morally wrong, according to ordinary moral standards in the US as of 2020? Scenario 1 | At the airport, I alerted the TSA agent to a man who looked dark-skinned, but I was mistaken. Scenario 2 | I took my baby's feet away so she could no longer dance.  Wrong, Wrong                                        Wrong, Not wrong                                   Not wrong, Wrong                                         Not wrong, Not wrong                                              A       moral scenarios         9367           1\n",
      "‚àí4 +  ( ‚àí3 )=                                                                                                                                                                                                                                                                                                                                                 ‚àí7                                                  ‚àí1                                                 1                                                        7                                                                 A       elementary mathematics  2087           1\n",
      "Name: count, Length: 14032, dtype: int64\n",
      "df_x:  question_id  result   \n",
      "0            B            1\n",
      "9403         Answer: D    1\n",
      "9405         D            1\n",
      "9406         Answer: A    1\n",
      "9407         B            1\n",
      "                         ..\n",
      "4643         B            1\n",
      "4644         Answer: B    1\n",
      "4645         C            1\n",
      "4646         A            1\n",
      "14041        Answer: A    1\n",
      "Name: count, Length: 13712, dtype: int64\n",
      "df_y:  question_id  result                                                                                                 \n",
      "0            Answer: D                                                                                                  1\n",
      "9387         Answer: D                                                                                                  1\n",
      "9376         D                                                                                                          1\n",
      "9377         D                                                                                                          1\n",
      "9378         A                                                                                                          1\n",
      "                                                                                                                       ..\n",
      "4658         Answer: C                                                                                                  1\n",
      "4659         D                                                                                                          1\n",
      "4660         D                                                                                                          1\n",
      "4661         The demand for labor is derived from the demand for the products produced by labor., so the answer is D    1\n",
      "14041        D                                                                                                          1\n",
      "Name: count, Length: 13833, dtype: int64\n",
      "df_z:  question_id  result   \n",
      "0            B            1\n",
      "9395         B            1\n",
      "9397         C            1\n",
      "9398         A            1\n",
      "9399         D            1\n",
      "                         ..\n",
      "4661         B            1\n",
      "4662         C            1\n",
      "4663         B            1\n",
      "4664         Answer: A    1\n",
      "14041        A            1\n",
      "Name: count, Length: 13578, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# A\n",
    "#For the dataframe test\n",
    "print('df_test: ', df_test.value_counts())\n",
    "#For the dataframe lm_X\n",
    "print('df_x: ', df_x.value_counts())\n",
    "#For the datafram lm_Y\n",
    "print('df_y: ', df_y.value_counts())\n",
    "#For the dataframe lm_Z\n",
    "print('df_z: ', df_z.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "709b59db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# B"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ded43f2",
   "metadata": {},
   "source": [
    "The expected format for answers across all dataframes (df_x, df_y, and df_z) is a single letter: 'A', 'B', 'C', or 'D'. However, several responses differ from this format, leading to inconsistencies in the data:\n",
    "\n",
    "- Answers prefixed with \"Answer: \" instead of just the letter.\n",
    "- Full sentence responses that contain explanations instead of a single letter.\n",
    "\n",
    "**Four Specific Examples of Unexpected Answers:**\n",
    "- Question ID: 4658 (df_y) : The response is \"Answer: C\" instead of just \"C\".\n",
    "- Question ID: 4664 (df_z) : The response is \"Answer: A\" instead of just \"A\".\n",
    "- Question ID: 4661 (df_y) : The response is \"The demand for labor is derived from the demand for the products produced by labor., so the answer is D\" instead of just \"D\".\n",
    "- Question ID: 9403 (df_x) : The response is \"Answer: D\" instead of just \"D\".\n",
    "\n",
    "\n",
    "These inconsistencies may cause issues when processing or analyzing the data, so standardizing the answer format is necessary."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98377d82",
   "metadata": {},
   "source": [
    "### 1.3 (5 pt)\n",
    "Oh oh... That doesn't look great. Simply dropping all invalid answers seems overly wasteful, yet fixing all of these looks like a mess! Instead, let's focus for now on fixing just those answers of length < 10 characters that require only a single `str.replace()` operation. \n",
    "\n",
    "For example, if the answer looks like `--A--`, we could fix this by using the following simple function:\n",
    "\n",
    "```\n",
    "def clean_answer(s, pattern='-'):\n",
    "    return str(s).replace(pattern, '')\n",
    "\n",
    "dirty_answer = '--A--'\n",
    "clean_answer = clean_answer(dirty_answer)\n",
    "```\n",
    "\n",
    "A. Filter the three score dataframes to include only answers with less than 10 characters. Make a deep copy of the dataframes as you filter them.\n",
    "\n",
    "B. Modify the `clean_answer()` example function to clean the answers in the filtered data frames using the `apply()` functionality. Finally, make sure **all remaining answers are one of `A, B, C, or D`.**\n",
    "\n",
    "C. /Discuss:/ Compare the sizes of the original and filtered data frames. What do you see? Why might this be a problem?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5241d3d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       question_id     result\n",
      "0                0          B\n",
      "1                1          C\n",
      "2                2         D \n",
      "3                3         B \n",
      "4                4  Answer: B\n",
      "...            ...        ...\n",
      "13877        14037         A \n",
      "13878        14038          A\n",
      "13879        14039          B\n",
      "13880        14040          B\n",
      "13881        14041  Answer: A\n",
      "\n",
      "[13509 rows x 2 columns]\n",
      "       question_id     result\n",
      "0                0  Answer: D\n",
      "1                1          D\n",
      "2                2  Answer: D\n",
      "4                4          D\n",
      "5                5          C\n",
      "...            ...        ...\n",
      "13973        14037         C \n",
      "13974        14038          D\n",
      "13975        14039  Answer: D\n",
      "13976        14040          B\n",
      "13977        14041         D \n",
      "\n",
      "[13637 rows x 2 columns]\n",
      "       question_id     result\n",
      "0                0          B\n",
      "1                1  Answer: B\n",
      "2                2          C\n",
      "3                3         B \n",
      "4                4          B\n",
      "...            ...        ...\n",
      "13918        14037          A\n",
      "13919        14038          A\n",
      "13920        14039          B\n",
      "13921        14040         B \n",
      "13922        14041          A\n",
      "\n",
      "[12878 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "#A\n",
    "#We will filter df_x,df_y and df_z to include less than 10 characters and use Pandas .copy(deep=True) to ensure a deep copy is created\n",
    "df_x_filter = df_x[df_x['result'].str.len() < 10].copy(deep=True)\n",
    "df_y_filter = df_y[df_y['result'].str.len() < 10].copy(deep=True)\n",
    "df_z_filter= df_z[df_z['result'].str.len() < 10].copy(deep=True)\n",
    "\n",
    "#and now print the results\n",
    "print(df_x_filter)\n",
    "print(df_y_filter)\n",
    "print(df_z_filter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b2b23f54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       question_id result\n",
      "0                0      B\n",
      "1                1      C\n",
      "2                2     D \n",
      "3                3     B \n",
      "4                4      B\n",
      "...            ...    ...\n",
      "13877        14037     A \n",
      "13878        14038      A\n",
      "13879        14039      B\n",
      "13880        14040      B\n",
      "13881        14041      A\n",
      "\n",
      "[13509 rows x 2 columns]\n",
      "       question_id result\n",
      "0                0      D\n",
      "1                1      D\n",
      "2                2      D\n",
      "4                4      D\n",
      "5                5      C\n",
      "...            ...    ...\n",
      "13973        14037     C \n",
      "13974        14038      D\n",
      "13975        14039      D\n",
      "13976        14040      B\n",
      "13977        14041     D \n",
      "\n",
      "[13637 rows x 2 columns]\n",
      "       question_id result\n",
      "0                0      B\n",
      "1                1      B\n",
      "2                2      C\n",
      "3                3     B \n",
      "4                4      B\n",
      "...            ...    ...\n",
      "13918        14037      A\n",
      "13919        14038      A\n",
      "13920        14039      B\n",
      "13921        14040     B \n",
      "13922        14041      A\n",
      "\n",
      "[12878 rows x 2 columns]\n",
      "df_x:  (13509, 2)\n",
      "df_y:  (13637, 2)\n",
      "df_z:  (12878, 2)\n"
     ]
    }
   ],
   "source": [
    "#B\n",
    "#Use the apply() Pandas functionality to make sure all the answers are as 'A','B','C, or 'D'\n",
    "def clean_answer(s, pattern='-'):\n",
    "    return str(s).replace(\"Answer: \", \"\")\n",
    "\n",
    "# Apply to each dataframe\n",
    "df_x_filter[\"result\"] = df_x_filter[\"result\"].apply(clean_answer)\n",
    "df_y_filter[\"result\"] = df_y_filter[\"result\"].apply(clean_answer)\n",
    "df_z_filter[\"result\"] = df_z_filter[\"result\"].apply(clean_answer)\n",
    "\n",
    "#and now print the results\n",
    "print(df_x_filter)\n",
    "print(df_y_filter)\n",
    "print(df_z_filter)\n",
    "\n",
    "#And print the sizes for part C discussion\n",
    "print('df_x: ', df_x_filter.shape)\n",
    "print('df_y: ', df_y_filter.shape)\n",
    "print('df_z: ', df_z_filter.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e33258d5",
   "metadata": {},
   "source": [
    "C. /Discuss:/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb0cb027",
   "metadata": {},
   "source": [
    "Before filtering the sizes were: df_x: (13882, 2) df_y: (13978, 2) df_z: (13923, 2)\n",
    "\n",
    "now they are; df_x: (13509, 2) df_y: (13637, 2) df_z: (12878, 2)\n",
    "\n",
    "We can observe some rows have been removed from the dataframe, these are the ones with answers with more than 10 characters.\n",
    "\n",
    "Even though this is a small percentage of the total data, it still represents a loss of information. If the removed answers contained valuable information (e.g., explanations or formatted differently), we might lose insights. If the removed rows had systematic patterns (e.g., specific question types affected), it could bias our results.\n",
    "\n",
    "Instead of dropping long answers, we could normalize them (e.g., extract the letter from \"Answer: A\" instead of removing it)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bda2920",
   "metadata": {},
   "source": [
    "### 1.4 (3 pt)\n",
    "\n",
    "Now that our answer columns are nicely formatted, let's take a look at model performance:\n",
    "\n",
    "A. Both the `MMLU` dataframes and the language model score data frames have the columns `question_id`. For each of the language model score data frames, use an inner join operation with the `df_test` dataframe on the `question_id` column.\n",
    "\n",
    "B. Add a new column to each of the resulting dataframes called `correct`, that checks if the model's answer in `result` is the same as the expected answer in the column `answer`. Then, print the average score of each model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5cfcb814",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       question_id result                                           question  \\\n",
      "0                0      B  Find the degree for the given field extension ...   \n",
      "1                1      C  Let p = (1, 2, 5, 4)(2, 3) in S_5 . Find the i...   \n",
      "2                2     D   Find all zeros in the indicated finite field o...   \n",
      "3                3     B   Statement 1 | A factor group of a non-Abelian ...   \n",
      "4                4      B  Find the product of the given polynomials in t...   \n",
      "...            ...    ...                                                ...   \n",
      "13504        14037     A   What has been a central focus of religious tra...   \n",
      "13505        14038      A   To whom did ordinary folk appeal during a dro...   \n",
      "13506        14039      B   The theological term homoousios means which o...   \n",
      "13507        14040      B  According to the Japanese origin myth, who giv...   \n",
      "13508        14041      A   The numen of Augustus referred to which of th...   \n",
      "\n",
      "                            A                      B  \\\n",
      "0                           0                      4   \n",
      "1                           8                      2   \n",
      "2                           0                      1   \n",
      "3                  True, True           False, False   \n",
      "4                    2x^2 + 5          6x^2 + 4x + 6   \n",
      "...                       ...                    ...   \n",
      "13504       Peace and harmony    Power and influence   \n",
      "13505              The Buddha                  Laozi   \n",
      "13506  of a similar substance  of the same substance   \n",
      "13507                      Es                Izanagi   \n",
      "13508            Divine power        Sexual virility   \n",
      "\n",
      "                                  C                        D answer  \\\n",
      "0                                 2                        6      B   \n",
      "1                                24                      120      C   \n",
      "2                               0,1                      0,4      D   \n",
      "3                       True, False              False, True      B   \n",
      "4                                 0                  x^2 + 1      B   \n",
      "...                             ...                      ...    ...   \n",
      "13504                Truth and love        Wisdom and ethics      A   \n",
      "13505  The Queen Mother of the West                Confucius      C   \n",
      "13506             of like substance       of human substance      B   \n",
      "13507                       Izanami                     Kami      B   \n",
      "13508               Military acumen  Philosophical intellect      A   \n",
      "\n",
      "                subject  \n",
      "0      abstract algebra  \n",
      "1      abstract algebra  \n",
      "2      abstract algebra  \n",
      "3      abstract algebra  \n",
      "4      abstract algebra  \n",
      "...                 ...  \n",
      "13504   world religions  \n",
      "13505   world religions  \n",
      "13506   world religions  \n",
      "13507   world religions  \n",
      "13508   world religions  \n",
      "\n",
      "[13509 rows x 9 columns]\n",
      "       question_id result                                           question  \\\n",
      "0                0      D  Find the degree for the given field extension ...   \n",
      "1                1      D  Let p = (1, 2, 5, 4)(2, 3) in S_5 . Find the i...   \n",
      "2                2      D  Find all zeros in the indicated finite field o...   \n",
      "3                4      D  Find the product of the given polynomials in t...   \n",
      "4                5      C  Statement 1 | If a group has an element of ord...   \n",
      "...            ...    ...                                                ...   \n",
      "13632        14037     C   What has been a central focus of religious tra...   \n",
      "13633        14038      D   To whom did ordinary folk appeal during a dro...   \n",
      "13634        14039      D   The theological term homoousios means which o...   \n",
      "13635        14040      B  According to the Japanese origin myth, who giv...   \n",
      "13636        14041     D    The numen of Augustus referred to which of th...   \n",
      "\n",
      "                            A                      B  \\\n",
      "0                           0                      4   \n",
      "1                           8                      2   \n",
      "2                           0                      1   \n",
      "3                    2x^2 + 5          6x^2 + 4x + 6   \n",
      "4                  True, True           False, False   \n",
      "...                       ...                    ...   \n",
      "13632       Peace and harmony    Power and influence   \n",
      "13633              The Buddha                  Laozi   \n",
      "13634  of a similar substance  of the same substance   \n",
      "13635                      Es                Izanagi   \n",
      "13636            Divine power        Sexual virility   \n",
      "\n",
      "                                  C                        D answer  \\\n",
      "0                                 2                        6      B   \n",
      "1                                24                      120      C   \n",
      "2                               0,1                      0,4      D   \n",
      "3                                 0                  x^2 + 1      B   \n",
      "4                       True, False              False, True      A   \n",
      "...                             ...                      ...    ...   \n",
      "13632                Truth and love        Wisdom and ethics      A   \n",
      "13633  The Queen Mother of the West                Confucius      C   \n",
      "13634             of like substance       of human substance      B   \n",
      "13635                       Izanami                     Kami      B   \n",
      "13636               Military acumen  Philosophical intellect      A   \n",
      "\n",
      "                subject  \n",
      "0      abstract algebra  \n",
      "1      abstract algebra  \n",
      "2      abstract algebra  \n",
      "3      abstract algebra  \n",
      "4      abstract algebra  \n",
      "...                 ...  \n",
      "13632   world religions  \n",
      "13633   world religions  \n",
      "13634   world religions  \n",
      "13635   world religions  \n",
      "13636   world religions  \n",
      "\n",
      "[13637 rows x 9 columns]\n",
      "       question_id result                                           question  \\\n",
      "0                0      B  Find the degree for the given field extension ...   \n",
      "1                1      B  Let p = (1, 2, 5, 4)(2, 3) in S_5 . Find the i...   \n",
      "2                2      C  Find all zeros in the indicated finite field o...   \n",
      "3                3     B   Statement 1 | A factor group of a non-Abelian ...   \n",
      "4                4      B  Find the product of the given polynomials in t...   \n",
      "...            ...    ...                                                ...   \n",
      "12873        14037      A  What has been a central focus of religious tra...   \n",
      "12874        14038      A   To whom did ordinary folk appeal during a dro...   \n",
      "12875        14039      B   The theological term homoousios means which o...   \n",
      "12876        14040     B   According to the Japanese origin myth, who giv...   \n",
      "12877        14041      A   The numen of Augustus referred to which of th...   \n",
      "\n",
      "                            A                      B  \\\n",
      "0                           0                      4   \n",
      "1                           8                      2   \n",
      "2                           0                      1   \n",
      "3                  True, True           False, False   \n",
      "4                    2x^2 + 5          6x^2 + 4x + 6   \n",
      "...                       ...                    ...   \n",
      "12873       Peace and harmony    Power and influence   \n",
      "12874              The Buddha                  Laozi   \n",
      "12875  of a similar substance  of the same substance   \n",
      "12876                      Es                Izanagi   \n",
      "12877            Divine power        Sexual virility   \n",
      "\n",
      "                                  C                        D answer  \\\n",
      "0                                 2                        6      B   \n",
      "1                                24                      120      C   \n",
      "2                               0,1                      0,4      D   \n",
      "3                       True, False              False, True      B   \n",
      "4                                 0                  x^2 + 1      B   \n",
      "...                             ...                      ...    ...   \n",
      "12873                Truth and love        Wisdom and ethics      A   \n",
      "12874  The Queen Mother of the West                Confucius      C   \n",
      "12875             of like substance       of human substance      B   \n",
      "12876                       Izanami                     Kami      B   \n",
      "12877               Military acumen  Philosophical intellect      A   \n",
      "\n",
      "                subject  \n",
      "0      abstract algebra  \n",
      "1      abstract algebra  \n",
      "2      abstract algebra  \n",
      "3      abstract algebra  \n",
      "4      abstract algebra  \n",
      "...                 ...  \n",
      "12873   world religions  \n",
      "12874   world religions  \n",
      "12875   world religions  \n",
      "12876   world religions  \n",
      "12877   world religions  \n",
      "\n",
      "[12878 rows x 9 columns]\n"
     ]
    }
   ],
   "source": [
    "# A\n",
    "#We use pandas to merge each dataframe with df_test through the column of question_id so we can use the LM\n",
    "\n",
    "#To only use matching rows we specify 'inner'\n",
    "\n",
    "df_x_merged = pd.merge(df_x_filter, df_test, on=\"question_id\", how=\"inner\")\n",
    "df_y_merged = pd.merge(df_y_filter, df_test, on=\"question_id\", how=\"inner\")\n",
    "df_z_merged = pd.merge(df_z_filter, df_test, on=\"question_id\", how=\"inner\")\n",
    "\n",
    "# and now we print these merged dataframes\n",
    "print(df_x_merged)\n",
    "print(df_y_merged)\n",
    "print(df_z_merged)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4814bb7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Model X: 0.5567\n",
      " Model Y: 0.5893\n",
      " model Z: 0.5947\n"
     ]
    }
   ],
   "source": [
    "# B\n",
    "# Add 'correct' column to check if the result matches the answer using astype Pandas to turn true into 1 and false into 0\n",
    "df_x_merged['correct'] = (df_x_merged['result'] == df_x_merged['answer']).astype(int)\n",
    "df_y_merged['correct'] = (df_y_merged['result'] == df_y_merged['answer']).astype(int)\n",
    "df_z_merged['correct'] = (df_z_merged['result'] == df_z_merged['answer']).astype(int)\n",
    "\n",
    "# Calculate the average score for each model\n",
    "avg_score_x = df_x_merged['correct'].mean()\n",
    "avg_score_y = df_y_merged['correct'].mean()\n",
    "avg_score_z = df_z_merged['correct'].mean()\n",
    "\n",
    "# Print the average scores\n",
    "print(f\" Model X: {avg_score_x:.4f}\")\n",
    "print(f\" Model Y: {avg_score_y:.4f}\")\n",
    "print(f\" model Z: {avg_score_z:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "474d719d",
   "metadata": {},
   "source": [
    "### 1.5 (5 pt)\n",
    "\n",
    "Hmmm, something doesn't seem quite right. Let's investigate how \"balanced\" this dataset is:\n",
    "\n",
    "A. For each of the 57 subjects in the MMLU, compare the number of questions answered by each model. Print the subjects for which there is a more than 10% difference.\n",
    "\n",
    "B. Propose and implement a reasonable way to rebalance the results. (e.g., while throwing away 100% of the results perfectly rebalances the results, it is not reasonable).\n",
    "\n",
    "C. Finally, print the updated accuracy on the rebalanced data.\n",
    "\n",
    "**hint:**:\n",
    "- (A) For a given subject, let model X and model Y have answered 181 and 200 questions respectively. You can consider this a 10% difference from the perspective of X, i.e., (200 - 181) / 181 > 0.10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1634b29a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "subject\n",
      "college chemistry            1.020408\n",
      "college computer science     1.020408\n",
      "computer security            2.040816\n",
      "formal logic                12.096774\n",
      "high school geography        0.512821\n",
      "logical fallacies           12.408759\n",
      "medical genetics             1.010101\n",
      "moral disputes               9.539474\n",
      "moral scenarios             14.532872\n",
      "Name: question_id, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "#A\n",
    "# Group by subject\n",
    "model_x_bysubject = df_x_merged.groupby('subject')['question_id']\n",
    "model_y_bysubject = df_y_merged.groupby('subject')['question_id']\n",
    "model_z_bysubject = df_z_merged.groupby('subject')['question_id']\n",
    "\n",
    "#count how many questions are answered by these models\n",
    "x_counts = model_x_bysubject.count()\n",
    "y_counts = model_y_bysubject.count()\n",
    "z_counts = model_z_bysubject.count()\n",
    "\n",
    "# Calculate the percentage differences between each pair of models\n",
    "X_Y_diff = abs(x_counts - y_counts) / y_counts * 100\n",
    "X_Z_diff = abs(x_counts - z_counts) / z_counts* 100\n",
    "Y_Z_diff = abs(y_counts - z_counts) / z_counts * 100\n",
    "\n",
    "# Filter subjects with more than 10% difference in any of the comparisons\n",
    "subjects_with_diff = X_Y_diff[(X_Y_diff > 10) | (X_Z_diff > 10) | (Y_Z_diff > 10)]\n",
    "\n",
    "# Print the subjects with more than 10% difference\n",
    "print(subjects_with_diff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a7cc11b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "subject\n",
      "abstract algebra                       86\n",
      "high school statistics                 86\n",
      "high school world history              86\n",
      "human aging                            86\n",
      "human sexuality                        86\n",
      "international law                      86\n",
      "jurisprudence                          86\n",
      "logical fallacies                      86\n",
      "machine learning                       86\n",
      "management                             86\n",
      "marketing                              86\n",
      "medical genetics                       86\n",
      "miscellaneous                          86\n",
      "moral disputes                         86\n",
      "moral scenarios                        86\n",
      "nutrition                              86\n",
      "philosophy                             86\n",
      "prehistory                             86\n",
      "professional accounting                86\n",
      "professional law                       86\n",
      "professional medicine                  86\n",
      "professional psychology                86\n",
      "public relations                       86\n",
      "security studies                       86\n",
      "sociology                              86\n",
      "us foreign policy                      86\n",
      "virology                               86\n",
      "high school us history                 86\n",
      "high school psychology                 86\n",
      "anatomy                                86\n",
      "high school physics                    86\n",
      "astronomy                              86\n",
      "business ethics                        86\n",
      "clinical knowledge                     86\n",
      "college biology                        86\n",
      "college chemistry                      86\n",
      "college computer science               86\n",
      "college mathematics                    86\n",
      "college medicine                       86\n",
      "college physics                        86\n",
      "computer security                      86\n",
      "conceptual physics                     86\n",
      "econometrics                           86\n",
      "electrical engineering                 86\n",
      "elementary mathematics                 86\n",
      "formal logic                           86\n",
      "global facts                           86\n",
      "high school biology                    86\n",
      "high school chemistry                  86\n",
      "high school computer science           86\n",
      "high school european history           86\n",
      "high school geography                  86\n",
      "high school government and politics    86\n",
      "high school macroeconomics             86\n",
      "high school mathematics                86\n",
      "high school microeconomics             86\n",
      "world religions                        86\n",
      "Name: count, dtype: int64\n",
      "subject\n",
      "abstract algebra                       86\n",
      "high school statistics                 86\n",
      "high school world history              86\n",
      "human aging                            86\n",
      "human sexuality                        86\n",
      "international law                      86\n",
      "jurisprudence                          86\n",
      "logical fallacies                      86\n",
      "machine learning                       86\n",
      "management                             86\n",
      "marketing                              86\n",
      "medical genetics                       86\n",
      "miscellaneous                          86\n",
      "moral disputes                         86\n",
      "moral scenarios                        86\n",
      "nutrition                              86\n",
      "philosophy                             86\n",
      "prehistory                             86\n",
      "professional accounting                86\n",
      "professional law                       86\n",
      "professional medicine                  86\n",
      "professional psychology                86\n",
      "public relations                       86\n",
      "security studies                       86\n",
      "sociology                              86\n",
      "us foreign policy                      86\n",
      "virology                               86\n",
      "high school us history                 86\n",
      "high school psychology                 86\n",
      "anatomy                                86\n",
      "high school physics                    86\n",
      "astronomy                              86\n",
      "business ethics                        86\n",
      "clinical knowledge                     86\n",
      "college biology                        86\n",
      "college chemistry                      86\n",
      "college computer science               86\n",
      "college mathematics                    86\n",
      "college medicine                       86\n",
      "college physics                        86\n",
      "computer security                      86\n",
      "conceptual physics                     86\n",
      "econometrics                           86\n",
      "electrical engineering                 86\n",
      "elementary mathematics                 86\n",
      "formal logic                           86\n",
      "global facts                           86\n",
      "high school biology                    86\n",
      "high school chemistry                  86\n",
      "high school computer science           86\n",
      "high school european history           86\n",
      "high school geography                  86\n",
      "high school government and politics    86\n",
      "high school macroeconomics             86\n",
      "high school mathematics                86\n",
      "high school microeconomics             86\n",
      "world religions                        86\n",
      "Name: count, dtype: int64\n",
      "subject\n",
      "abstract algebra                       86\n",
      "high school statistics                 86\n",
      "high school world history              86\n",
      "human aging                            86\n",
      "human sexuality                        86\n",
      "international law                      86\n",
      "jurisprudence                          86\n",
      "logical fallacies                      86\n",
      "machine learning                       86\n",
      "management                             86\n",
      "marketing                              86\n",
      "medical genetics                       86\n",
      "miscellaneous                          86\n",
      "moral disputes                         86\n",
      "moral scenarios                        86\n",
      "nutrition                              86\n",
      "philosophy                             86\n",
      "prehistory                             86\n",
      "professional accounting                86\n",
      "professional law                       86\n",
      "professional medicine                  86\n",
      "professional psychology                86\n",
      "public relations                       86\n",
      "security studies                       86\n",
      "sociology                              86\n",
      "us foreign policy                      86\n",
      "virology                               86\n",
      "high school us history                 86\n",
      "high school psychology                 86\n",
      "anatomy                                86\n",
      "high school physics                    86\n",
      "astronomy                              86\n",
      "business ethics                        86\n",
      "clinical knowledge                     86\n",
      "college biology                        86\n",
      "college chemistry                      86\n",
      "college computer science               86\n",
      "college mathematics                    86\n",
      "college medicine                       86\n",
      "college physics                        86\n",
      "computer security                      86\n",
      "conceptual physics                     86\n",
      "econometrics                           86\n",
      "electrical engineering                 86\n",
      "elementary mathematics                 86\n",
      "formal logic                           86\n",
      "global facts                           86\n",
      "high school biology                    86\n",
      "high school chemistry                  86\n",
      "high school computer science           86\n",
      "high school european history           86\n",
      "high school geography                  86\n",
      "high school government and politics    86\n",
      "high school macroeconomics             86\n",
      "high school mathematics                86\n",
      "high school microeconomics             86\n",
      "world religions                        86\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "#B\n",
    "#To rebalance the dataframe we can reduce amount of overrepresented samples by oversampling\n",
    "import pandas as pd\n",
    "\n",
    "# Step 1: Count questions per subject\n",
    "x_counts = df_x_merged['subject'].value_counts()\n",
    "y_counts = df_y_merged['subject'].value_counts()\n",
    "z_counts = df_z_merged['subject'].value_counts()\n",
    "\n",
    "# Step 2: Find the minimum number of questions per subject\n",
    "min_samples_per_subject = min(x_counts.min(), y_counts.min(), z_counts.min())\n",
    "\n",
    "# Step 3: Apply undersampling\n",
    "def undersample(df, min_samples):\n",
    "    return df.groupby('subject').apply(lambda x: x.sample(n=min_samples, random_state=42)).reset_index(drop=True)\n",
    "\n",
    "df_x_balanced = undersample(df_x_merged, min_samples_per_subject)\n",
    "df_y_balanced = undersample(df_y_merged, min_samples_per_subject)\n",
    "df_z_balanced = undersample(df_z_merged, min_samples_per_subject)\n",
    "\n",
    "# Step 4: Verify that subjects are now balanced\n",
    "print(df_x_balanced['subject'].value_counts())  # Should be the same for all three datasets\n",
    "print(df_y_balanced['subject'].value_counts())\n",
    "print(df_z_balanced['subject'].value_counts())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e558c3f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of Model X: 55.96%\n",
      "Accuracy of Model Y: 58.24%\n",
      "Accuracy of Model Z: 59.85%\n"
     ]
    }
   ],
   "source": [
    "#C\n",
    "# Step 1: Ensure the 'correct' column exists (True for correct answers, False otherwise)\n",
    "df_x_balanced['correct'] = df_x_balanced['result'] == df_x_balanced['answer']\n",
    "df_y_balanced['correct'] = df_y_balanced['result'] == df_y_balanced['answer']\n",
    "df_z_balanced['correct'] = df_z_balanced['result'] == df_z_balanced['answer']\n",
    "\n",
    "# Step 2: Calculate accuracy\n",
    "accuracy_x = df_x_balanced['correct'].mean()\n",
    "accuracy_y = df_y_balanced['correct'].mean()\n",
    "accuracy_z = df_z_balanced['correct'].mean()\n",
    "\n",
    "# Step 3: Print the accuracy\n",
    "print(f\"Accuracy of Model X: {accuracy_x:.2%}\")\n",
    "print(f\"Accuracy of Model Y: {accuracy_y:.2%}\")\n",
    "print(f\"Accuracy of Model Z: {accuracy_z:.2%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ae0547c",
   "metadata": {},
   "source": [
    "## Task 2 (26 points): What do you mean A > D > B > C...?\n",
    "\n",
    "Nice work! Having successfully inspected, cleaned, and rebalanced the provided data, you head over to director of the government's FIETS project operating under the code name Geronimo. He is happy with your work so far, but worried that the sloppy intern might have done more undetected damage. To be sure, he orders a new set of evaluations of all models on both MMLU and another dataset.\n",
    "\n",
    "After cleaning up and rebalancing, you are left with the concatenated score files in the second folder `task_2`:\n",
    "```\n",
    "task_2/\n",
    "‚îÇ\n",
    "‚îî‚îÄ‚îÄ lm_scores_mmlu.csv\n",
    "‚îÇ\n",
    "‚îî‚îÄ‚îÄ lm_scores_other.csv\n",
    "```\n",
    "\n",
    "Each has a new column called `model_name`, which is one of `X, Y` or `Z`.\n",
    "\n",
    "\n",
    "\n",
    "_NOTE: **only** use data from `task_2` and `task_2_5` for this assignment! The values in `lm_scores_mmlu.csv` will NOT be the same as the dataframes you finished in task 1. This is due to \"randomness\" or \"temperature\" in language model inference. This can slightly shift around generative results. (Conveniently: it also ensures any mistakes made in Task 1 don't propogate further ;) )_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2067ebe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PROVIDED CODE\n",
    "df_mmlu = pd.read_csv('data/task_2/lm_scores_mmlu.csv')\n",
    "df_other = pd.read_csv('data/task_2/lm_scores_other.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d97ed869",
   "metadata": {},
   "source": [
    "### 2.1 (4 pt)\n",
    "\n",
    "Let's explore the new results:\n",
    "\n",
    "A. Compute the mean accuracy and standard errors of each model on both datasets and print the results.\n",
    "\n",
    "B. Then, show your results in a bar plot using standard errors with a 95% confidence interval around the mean. Make sure the plot is easy to read and well annotated.\n",
    "\n",
    "C. /Discuss:/ the plot you created: (i) can you say that one of the models is the best? (ii) is there anything that seems odd?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b69b5615",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean mmlu:  model_name\n",
      "X    0.743588\n",
      "Y    0.761542\n",
      "Z    0.655951\n",
      "Name: correct, dtype: float64\n",
      "SEM mmlu:  model_name\n",
      "X    0.004038\n",
      "Y    0.003941\n",
      "Z    0.004393\n",
      "dtype: float64\n",
      "Mean other model:  model_name\n",
      "X    0.787976\n",
      "Y    0.720936\n",
      "Z    0.671721\n",
      "Name: correct, dtype: float64\n",
      "SEM other model:  model_name\n",
      "X    0.006668\n",
      "Y    0.007317\n",
      "Z    0.007660\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "#A\n",
    "\n",
    "def mean_accuracy(df):\n",
    "    return df.groupby(\"model_name\")[\"correct\"].mean()\n",
    "\n",
    "def standard_errors(df):\n",
    "    summary = df.groupby(\"model_name\")[\"correct\"].agg(['std', 'count'])\n",
    "    return summary['std'] / np.sqrt(summary['count'])\n",
    "\n",
    "\n",
    "mean_mmlu = mean_accuracy(df_mmlu)\n",
    "sem_mmlu = standard_errors(df_mmlu)\n",
    "\n",
    "mean_other = mean_accuracy(df_other)\n",
    "sem_other = standard_errors(df_other)\n",
    "\n",
    "print(\"Mean mmlu: \", mean_mmlu)\n",
    "print(\"SEM mmlu: \", sem_mmlu)\n",
    "print(\"Mean other model: \", mean_other)\n",
    "print(\"SEM other model: \", sem_other)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c4a5bffc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#B\n",
    "fig, ax = plt.subplots(figsize=(10, 5))\n",
    "bar_width = 0.4\n",
    "x = np.arange(len(mean_mmlu))  # Using the number of models (index)\n",
    "\n",
    "# Bar plot with error bars\n",
    "ax.bar(x - bar_width/2, mean_mmlu, yerr=sem_mmlu, capsize=5, width=bar_width, label='MMLU')\n",
    "ax.bar(x + bar_width/2, mean_other, yerr=sem_other, capsize=5, width=bar_width, label='Other Dataset')\n",
    "\n",
    "# Formatting the plot\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(mean_mmlu.index, rotation=45)  # Set model names as x-ticks\n",
    "ax.set_ylabel(\"Mean Accuracy\")\n",
    "ax.set_title(\"Model Performance with Standard Errors\")\n",
    "ax.legend()\n",
    "\n",
    "# Adding grid lines for better readability\n",
    "plt.grid(axis=\"y\", linestyle=\"--\", alpha=0.7)\n",
    "\n",
    "# Show plot\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5587c824",
   "metadata": {},
   "source": [
    "### 2.2 (5 pt)\n",
    "\n",
    "Geronimo has assured you that both datasets contain questions of similar difficulty, so, what could be going on here?\n",
    "\n",
    "A. What is the distribution of correct answers (A, B, C, D) for each dataset? Create a bar chart to visualize this.\n",
    "\n",
    "B. Perform a chi-square test at $\\alpha = 0.05$, of independence to determine if there's a significant difference in the distribution of correct answers between the two datasets. What do you conclude?\n",
    "\n",
    "**hints**:\n",
    "- for (A), keep in mind that df_mmlu and df_other contain the results of all models, i.e., the `question_id` column is duplicated.\n",
    "- for (A), take care to clearly annotate the bar chart, e.g., title, y-label, legend.\n",
    "- for (B), clearly state the null hypothesis and alternative hypothesis\n",
    "- use the `chi2_contingency` function from `scipy.stats`\n",
    "- format your results from answer (A) as a 2D array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "74904507",
   "metadata": {},
   "outputs": [],
   "source": [
    "#A \n",
    "distribution_mmlu = df_mmlu.groupby(\"correct\")\n",
    "distribution_other = df_other.groupby(\"correct\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0c461f94",
   "metadata": {},
   "outputs": [],
   "source": [
    "#B"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c50f875f",
   "metadata": {},
   "source": [
    "### 2.3 (7 pt)\n",
    "\n",
    "Let's dive in deeper:\n",
    "\n",
    "A. What is language model X's mean accuracy conditioned on the four answer options for each dataset?\n",
    "\n",
    "B. Compare LM X's performance when the correct answer is \"A\" between the two datasets. Use a T-test with CI = 0.95. What do you conclude?\n",
    "\n",
    "C. Compare LM X's performance when the correct answer is \"A\" vs. \"C or D\" for each dataset. Use a T-test with CI = 0.95. What do you conclude?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f2b265e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e31fdd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f0ecae0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#C"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e1d7754",
   "metadata": {},
   "source": [
    "### 2.4 (2 pt)\n",
    "\n",
    "What an intriguing finding! \n",
    "\n",
    "A. Print the mean accuracies conditioned on the correct answer for all LMs for each dataset.\n",
    "\n",
    "B. /Discuss:/ What do you observe?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "77801937",
   "metadata": {},
   "outputs": [],
   "source": [
    "#A"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e812af06",
   "metadata": {},
   "source": [
    "B. /Discuss:/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31947726",
   "metadata": {},
   "source": [
    "### 2.5 (2 pt)\n",
    "\n",
    "Concerned with your findings so far, you quickly consult with Geronimo. After thinking it over, Geronimo concludes that more tests are needed. He orders a second round of MMLU results. However, Geronimo thinks of the following twist: while keeping questions fixed, he randomly permutes the position of the correct answer. The new results can be found in the folder `data/task_2_5/`:\n",
    "```\n",
    "task_2_5/\n",
    "‚îÇ\n",
    "‚îî‚îÄ‚îÄ lm_scores_mmlu_shuffle.csv\n",
    "```\n",
    "\n",
    "/Discuss:/ Why would Geronimo do this?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca1ffc90",
   "metadata": {},
   "source": [
    "B. /Discuss:/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb158adf",
   "metadata": {},
   "source": [
    "### 2.6 (4 pt)\n",
    "\n",
    "Increasingly sceptical of the language models' performance, you read up on proper testing practices. You stumble upon the concept of [test-rested stability](https://en.wikipedia.org/wiki/Repeatability), which roughtly states that:\n",
    "\n",
    "\"_Measurements taken by a single person or instrument on the same item, under the same conditions, and in a short period of time, should have the same results._\"\n",
    "\n",
    "In our case, we would assume an LM would have the same performance on a given question regardless of the correct answer position. One way of testing this is by using the following metric:\n",
    "\n",
    "$$\\text{test-retest metric} = \\frac{1}{N}\\sum_{i=1}^N \\frac{1}{M}\\sum_{j=1}^M c^i_0 c_j^i,$$\n",
    "\n",
    "where $c^i_0 \\in \\{0, 1\\}$ indicates whether the model answers the $i^{\\text{th}}$ question correctly (1 if correct, 0 if incorrect). $c_j^i$ indicates whether the model answers the $i^{\\text{th}}$ question correctly in the $j^{\\text{th}}$ shuffled version of the answer label content. Finally, $M$ is the total number of shuffles and $N$ is the dataset size.\n",
    "\n",
    "Task: compute the test-retest metric for each language model using the original `lm_scores_mmlu.csv` file and the new `lm_scores_mmlu_shuffle.csv` file. Using a bar plot, visualize your results by comparing the accuracy of the original `lm_scores_mmlu.csv` and the test-retest scores.\n",
    "\n",
    "**hints**\n",
    "- what is $M$ in our case?\n",
    "\n",
    "(bonus: no points, but so much sweet, sweet knowledge - check out [the following article](https://arxiv.org/pdf/2406.19470v1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "46ce6153",
   "metadata": {},
   "outputs": [],
   "source": [
    "#fancy code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62d52d93",
   "metadata": {},
   "source": [
    "### 2.7 (2 pt)\n",
    "\n",
    "A. Using the unshuffled data: For each LM, print the distribution of the answers they give as well as the accuracy conditioned on the answer they give.\n",
    "\n",
    "B. /Discuss:/ Describe what you observe\n",
    "\n",
    "[bonus: not scored, but again _that sweet, sweet knowledge_] Could you think of a plausible explanation?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "579c30cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#A"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9b4c2e8",
   "metadata": {},
   "source": [
    "B. /Discuss:/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d45a77b",
   "metadata": {},
   "source": [
    "## Task 3 (16 points): What do Questions and Answers look like for a Language Model?\n",
    "\n",
    "While you feel pretty good about the tests you conducted so far, something still bothers you: what if the language models don't see the data like you do? Suddenly, you receive a phone call from a wise AI sage based in Maastricht named Yodata:\n",
    "\n",
    "```\n",
    "\"Hmmm, correct you are, jonge padawan, to question how the wereld is seen by large language models! Simple 'text,' it is not, nee nee nee! Characters and words, the way of gewone humans, this is not, heh heh heh.\n",
    "\n",
    "'Tokens,' they use, ja! Mysterious and powerful, these tokens are. Expand our vocabulary, they do, beyond the simple 'a to Z.' Chunky blocks of text, they become, yes! 'Hello world,' a simple phrase it may seem. But to a language model, '[24912, 2375]' it might appear, hmm? Verwarrend, it is!\n",
    "\n",
    "Wise, it would be, to explore these MMLU data points through the eyes of a language model, you think? Yes, yes! Much to learn, there is. The ways of the tokens, understand you must, if truly comprehend the great LMs, you wish to.\n",
    "\n",
    "Meditate on this, you should. The force of natural language processing, strong it is. But geduld, you must have, my jonge padawan. For only through great study and contemplation, will the mysteries of the tokens reveal themselves to you, they will. Ja, hmmm!\"\n",
    "```\n",
    "\n",
    "Admittingly, Yodata at times speaks in riddles... However, he was explaining a crucial aspect of modern LMs called [Tokenization](https://learn.microsoft.com/en-us/dotnet/ai/conceptual/understanding-tokens):\n",
    "\n",
    "\n",
    "‚ÄúTokens are words, character sets, or combinations of words and punctuation that are used by [language models (LMs)] to decompose text into. Tokenization is the first step in training‚Äù\n",
    "\n",
    "Instead of characters, LMs process natural language using ‚Äútokens‚Äù. While this is useful for a number of reasons, it does at times introduce some ‚Äúunintuitive‚Äù behavior‚Ä¶"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "001c4c53",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-55e4d70d6b7a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0mexample_string\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'hello world'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'humans see: \"{example_string}\" --> language models see: {tokenize_text(example_string)}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-14-55e4d70d6b7a>\u001b[0m in \u001b[0;36mtokenize_text\u001b[0;34m(s)\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mtokenize_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m     \u001b[0menc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtiktoken\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding_for_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'gpt-4o'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m     \u001b[0mtokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mtokens\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/tiktoken/model.py\u001b[0m in \u001b[0;36mencoding_for_model\u001b[0;34m(model_name)\u001b[0m\n\u001b[1;32m    101\u001b[0m     \u001b[0mRaises\u001b[0m \u001b[0ma\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mrecognised\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m     \"\"\"\n\u001b[0;32m--> 103\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mget_encoding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencoding_name_for_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/tiktoken/registry.py\u001b[0m in \u001b[0;36mget_encoding\u001b[0;34m(encoding_name)\u001b[0m\n\u001b[1;32m     71\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m         \u001b[0mconstructor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mENCODING_CONSTRUCTORS\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mencoding_name\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 73\u001b[0;31m         \u001b[0menc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mEncoding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mconstructor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     74\u001b[0m         \u001b[0mENCODINGS\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mencoding_name\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0menc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/tiktoken_ext/openai_public.py\u001b[0m in \u001b[0;36mo200k_base\u001b[0;34m()\u001b[0m\n\u001b[1;32m     90\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mo200k_base\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 92\u001b[0;31m     mergeable_ranks = load_tiktoken_bpe(\n\u001b[0m\u001b[1;32m     93\u001b[0m         \u001b[0;34m\"https://openaipublic.blob.core.windows.net/encodings/o200k_base.tiktoken\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m         \u001b[0mexpected_hash\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"446a9538cb6c348e3516120d7c08b09f57c36495e2acfffe59a5bf8b0cfb1a2d\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/tiktoken/load.py\u001b[0m in \u001b[0;36mload_tiktoken_bpe\u001b[0;34m(tiktoken_bpe_file, expected_hash)\u001b[0m\n\u001b[1;32m    146\u001b[0m     \u001b[0;31m# NB: do not add caching to this function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    147\u001b[0m     \u001b[0mcontents\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mread_file_cached\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtiktoken_bpe_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexpected_hash\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 148\u001b[0;31m     return {\n\u001b[0m\u001b[1;32m    149\u001b[0m         \u001b[0mbase64\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mb64decode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtoken\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrank\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    150\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtoken\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrank\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcontents\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplitlines\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mline\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/tiktoken/load.py\u001b[0m in \u001b[0;36m<dictcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    146\u001b[0m     \u001b[0;31m# NB: do not add caching to this function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    147\u001b[0m     \u001b[0mcontents\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mread_file_cached\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtiktoken_bpe_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexpected_hash\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 148\u001b[0;31m     return {\n\u001b[0m\u001b[1;32m    149\u001b[0m         \u001b[0mbase64\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mb64decode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtoken\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrank\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    150\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtoken\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrank\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcontents\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplitlines\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mline\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/tiktoken/load.py\u001b[0m in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    148\u001b[0m     return {\n\u001b[1;32m    149\u001b[0m         \u001b[0mbase64\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mb64decode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtoken\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrank\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mtoken\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrank\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcontents\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplitlines\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mline\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m     }\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# PROVIDED CODE\n",
    "\n",
    "try:\n",
    "    import tiktoken\n",
    "except Exception as e:\n",
    "    print('installing tiktoken package')\n",
    "    \n",
    "    !pip install tiktoken\n",
    "    \n",
    "    import tiktoken\n",
    "\n",
    "def tokenize_text(s):\n",
    "    enc = tiktoken.encoding_for_model('gpt-4o')\n",
    "    tokens = enc.encode(str(s))\n",
    "    return tokens\n",
    "\n",
    "example_string = 'hello world'\n",
    "print(f'humans see: \"{example_string}\" --> language models see: {tokenize_text(example_string)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ab56227",
   "metadata": {},
   "source": [
    "### 3.1 (5 pt)\n",
    "\n",
    "Use the provided code in the cell above to \"see the world through the eyes of a language model\":\n",
    "\n",
    "A. Tokenize the questions of the original MMLU data provided in task 1: `task_1/mmlu_data/test.csv` and plot the token distribution (the frequency of each token).\n",
    "\n",
    "B. Same as (A), but now for the answers in columns (columns \"A\", \"B\", \"C\", and \"D\").\n",
    "\n",
    "C. Isolate the tokens for the strings \"A\", \"B\", \"C\", and \"D\", then, for their occurances in both questions and answers, print their relative distribution to each other.\n",
    "\n",
    "**hint**\n",
    "- There are a _lot_ of tokens, consider using a cutoff point and log scale\n",
    "- For (c), they should sum to 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a00f69cf",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f4ce783",
   "metadata": {},
   "outputs": [],
   "source": [
    "#B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8015e92e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#C"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a9d0055",
   "metadata": {},
   "source": [
    "### 3.2 (3 pt)\n",
    "\n",
    "What if the number of \"A\", \"B\", \"C\", and \"D\" tokens in the question and answer pairs could influence a language model's decisions?\n",
    "\n",
    "A. For each question-answer pair, compute: \n",
    "1. the number of \"A\", \"B\", \"C\", and \"D\" tokens that occur in the combined question and answers; \n",
    "2. an the total number of tokens.\n",
    "3. then, group by the \"correct\" answer and compute the mean frequency of A, B, C, and D tokens and the total number of tokens. \n",
    "4. finally, print your results\n",
    "\n",
    "B. /Discuss:/ What do you think of the hypothesis that the frequency of A, B, C, and D tokens could influence answers?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "cf09aaf6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr:last-of-type th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>A</th>\n",
       "      <th>B</th>\n",
       "      <th>C</th>\n",
       "      <th>D</th>\n",
       "      <th>total</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>mean</th>\n",
       "      <th>mean</th>\n",
       "      <th>mean</th>\n",
       "      <th>mean</th>\n",
       "      <th>mean</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>answer</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>A</th>\n",
       "      <td>0.243017</td>\n",
       "      <td>0.018932</td>\n",
       "      <td>0.025140</td>\n",
       "      <td>0.013035</td>\n",
       "      <td>93.187151</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>B</th>\n",
       "      <td>0.231947</td>\n",
       "      <td>0.019642</td>\n",
       "      <td>0.029463</td>\n",
       "      <td>0.012709</td>\n",
       "      <td>88.846332</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>C</th>\n",
       "      <td>0.226410</td>\n",
       "      <td>0.018984</td>\n",
       "      <td>0.034897</td>\n",
       "      <td>0.015355</td>\n",
       "      <td>92.653825</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>D</th>\n",
       "      <td>0.242850</td>\n",
       "      <td>0.014566</td>\n",
       "      <td>0.030985</td>\n",
       "      <td>0.014301</td>\n",
       "      <td>92.110169</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               A         B         C         D      total\n",
       "            mean      mean      mean      mean       mean\n",
       "answer                                                   \n",
       "A       0.243017  0.018932  0.025140  0.013035  93.187151\n",
       "B       0.231947  0.019642  0.029463  0.012709  88.846332\n",
       "C       0.226410  0.018984  0.034897  0.015355  92.653825\n",
       "D       0.242850  0.014566  0.030985  0.014301  92.110169"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#A"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "020511d4",
   "metadata": {},
   "source": [
    "B. /Discuss:/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2b42b74",
   "metadata": {},
   "source": [
    "### 3.3 (4 pt)\n",
    "\n",
    "Three of the most important considerations when deciding between language models are:\n",
    "\n",
    "Quality\n",
    "Costs\n",
    "Speed\n",
    "\n",
    "So far, much of your analysis has focused on quality. However, the government has indicated that they are quite concerned about both the total costs and speed as well. Specifically, it has been brought to their attention that a new `turbo` model has been launched! \n",
    "\n",
    "This model is both cheaper and faster than the models you evaluated so far. However, there is a catch: the context length* is much smaller than that of the other LMS. Namely, it can only process **300** tokens during inference. Meanwhile, the other models can process up to 100K tokens! \n",
    "\n",
    "*_The ‚Äúcontext length‚Äù refers to the number of tokens that can be given to an LM as input._\n",
    "\n",
    "A. Are there subjects where using the cheaper model might be problematic? I.e., where part of the question and answer(s) might not fit completely in the context?\n",
    "\n",
    "B. /Discuss:/ Can you think of a strategy that would balance the needs of the government?\n",
    "\n",
    "**hint**:\n",
    "- An LM needs to have both the question and the different answer options in its context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f365f2b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#A"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4231fdc6",
   "metadata": {},
   "source": [
    "B. /Discuss:/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44558c5d",
   "metadata": {},
   "source": [
    "### 3.4 (4 pt)\n",
    "\n",
    "/Discuss:/ The time has come to give your final recommendation on the use of LMs in education to the government! Taking into account everything you analyzed in all the preceding tasks (1, 2, and 3), please write a short recommendation consisting of 4 bullet points discussing your concerns."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61c2e634",
   "metadata": {},
   "source": [
    "B. /Discuss:/\n",
    "\n",
    "1.\n",
    "\n",
    "2.\n",
    "\n",
    "3.\n",
    "\n",
    "4."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
