{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e3c21e43",
   "metadata": {
    "colab_type": "text",
    "id": "i_f5u2x9nn6I",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Clinic 4 - Dimensionality Reduction (Bootcamp)\n",
    "\n",
    "### Learning goals\n",
    "\n",
    "After this clinic/bootcamp you should be able to:\n",
    "\n",
    "+ Explain why dimensionality reduction is important in data analysis and machine learning.\n",
    "+ Compare PCA and SVD as methods for reducing dimensionality.\n",
    "+ Identify scenarios where DR is beneficial, such as noise reduction and feature selection.\n",
    "+ Perform PCA on a dataset using Python\n",
    "+ Visualize variance explained by principal components (e.g., scree plot).\n",
    "+ Project high-dimensional data onto lower dimensions and analyze information retention.\n",
    "+ Compare PCA results before and after standardizing the dataset.\n",
    "+ Explain the meaning of principal components and their relation to original features.\n",
    "+ Identify the impact of different numbers of principal components on data representation.\n",
    "+ Assess reconstruction error when reducing dimensions.\n",
    "+ Apply PCA/SVD to real-world datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37182816",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Part 1a: What is Dimensionality Reduction (reminder from lecture, feel free to skip)\n",
    "\n",
    "Consider a dataset $\\mathcal{D} = \\{x^{(i)} \\mid i = 1,2,...,n\\}$ of motorcycles, characterized by a set of attributes.\n",
    "* Attributes include size, color, maximum speed, etc.\n",
    "* Suppose that two attributes are closely correlated: e.g., $x^{(i)}_j$ is the speed in `mph` and $x^{(i)}_k$ is the speed in `km/h`.\n",
    "* The real dimensionality of the data is $d-1$!\n",
    "\n",
    "We would like to automatically identify the right data dimensionality."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0583984f",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Dimensionality Reduction\n",
    "\n",
    "More generally, a dimensionality reduction algorithm learns from data an unsupervised model\n",
    "$$f_\\theta : \\mathbb{R}^d \\to \\mathbb{R}^p,$$\n",
    "where $\\mathbb{R}^p$ contains low-dimensional representation of the data $(p<d)$.\n",
    "\n",
    "For each input $x^{(i)}$, $f_\\theta$ computes a low-dimensional representation $z^{(i)}$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8acb3fab",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Linear Dimensionality Reduction\n",
    "\n",
    "The transformation \n",
    "$$f_\\theta : \\mathbb{R}^d \\to \\mathbb{R}^p$$\n",
    "is a linear function with parameters $\\theta = W \\in \\mathbb{R}^{d \\times p}$:\n",
    "$$ z = f_\\theta(x) = W^\\top \\cdot x. $$\n",
    "The latent dimension $z$ is obtained from $x$ via a matrix $W$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac552f57",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "## Example: Discovering Structure in Digits\n",
    "\n",
    "Dimensionality reduction can reveal interesting structure in digits without using labels.\n",
    "\n",
    "<center><img width=40% src=\"img/aae_dim_reduc_2.png\"></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad80760c",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "## Example: DNA Analysis\n",
    "\n",
    "Even linear dimensionality reduction is powerful. Here, in uncovers the geography of European countries from only DNA data\n",
    "\n",
    "<center><img width=50% src=\"img/dna_map.jpg\"></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdd3c907",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Other Kinds of Dimensionality Reduction\n",
    "\n",
    "We will focus on linear dimensionality reduction this lecture, but there exist many other methods:\n",
    "* Non-linear methods based on kernels (e.g., Kernel PCA)\n",
    "* Non-linear methods based on deep learning (e.g., variational autoencoders)\n",
    "* Non-linear methods based on maximizing signal independence (independent component analysis)\n",
    "* Probabilistic versions of the above\n",
    "\n",
    "\n",
    "See the `scikit-learn` [guide](https://scikit-learn.org/stable/modules/unsupervised_reduction.html) for more!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c860666c",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Part 1b: Principal Component Analysis (reminder from lecture, feel free to skip)\n",
    "\n",
    "We will now describe principal component analysis (PCA)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f674c31",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "## What Problem Does PCA Solve? Math.\n",
    "\n",
    "Our first step is to make this goal more precise. Suppose we have datapoints $x \\in \\mathbb{R}^{d}$.\n",
    "\n",
    "1. Finding a linear subspace means finding an (orthonormal) set of $p < d$ basis vectors \n",
    "\\begin{align}\n",
    "W = [ w^{(1)}, w^{(2)}, \\ldots, w^{(p)} ],\n",
    "\\end{align}\n",
    "where $W \\in \\mathbb{R}^{d \\times p}$ is the matrix of stacked vectors."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "386b76ad",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "2. Finding projections of the data in in the subspace means computing\n",
    "\\begin{align}\n",
    "z = \n",
    "\\begin{bmatrix}\n",
    "z_1 \\\\ z_2 \\\\ \\vdots \\\\ z_k\n",
    "\\end{bmatrix}=\n",
    "\\begin{bmatrix}\n",
    "(w^{(1)})^\\top x \\\\ (w^{(2)})^\\top x \\\\ \\vdots \\\\ (w^{(k)})^\\top x\n",
    "\\end{bmatrix}\n",
    "= W^\\top x \n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee4ab23f",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "We can choose a basis $W$ for the data plane. The coordinates in this basis are denoted by $z$ (image [credit](https://doc.plob.org/machine_learning/14_Dimensionality_Reduction.html)).\n",
    "\n",
    "<center><img width=80% src=\"img/pca_example.png\"></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb88fa6e",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Components of an Unsupervised Learning Algorithm\n",
    "\n",
    "We will define PCA in terms of the three standard components of an ML algorithm.\n",
    "\n",
    "$$ \\underbrace{\\text{Dataset}}_\\text{Attributes} + \\underbrace{\\text{Learning Algorithm}}_\\text{Model Class + Objective + Optimizer } \\to \\text{Unsupervised Model} $$\n",
    "\n",
    "The dataset $\\mathcal{D} = \\{x^{(i)} \\mid i = 1,2,...,n\\}$ does not include any labels."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "457d4c61",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## The PCA Model\n",
    "\n",
    "The model for PCA is a function $f_\\theta$ of the form\n",
    "$$ z = f_\\theta(x) = W^\\top x, $$\n",
    "where $\\theta = W$ and $W$ is a $d \\times p$ matrix of $p$ orthonormal column vectors denoted as $w^{(1)}, w^{(2)}, \\ldots, w^{(p)}$.\n",
    "\n",
    "Note that when $x = W z$, then $z = W^T x$ because $W^\\top W = I$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb476ed7",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "This model enables performing two tasks:\n",
    "* __Encoding__: $z^{(i)} = W^\\top x^{(i)}$, finding the low-dimensional form of $x$\n",
    "\\begin{align}\n",
    "z^{(i)} = \n",
    "\\begin{bmatrix}\n",
    "z^{(i)}_1 \\\\ z^{(i)}_2 \\\\ \\vdots \\\\ z^{(i)}_k\n",
    "\\end{bmatrix}=\n",
    "\\begin{bmatrix}\n",
    "(w^{(1)})^\\top x^{(i)} \\\\ (w^{(2)})^\\top x^{(i)} \\\\ \\vdots \\\\ (w^{(k)})^\\top x^{(i)}\n",
    "\\end{bmatrix}\n",
    "= W^\\top x^{(i)}\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "641196a9",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* __Decoding__: $\\tilde x^{(i)} = W  z^{(i)}$, converting a low-dimensional $z^{(i)}$ to a high-dimensional reconstruction $\\tilde x^{(i)}$ of $x^{(i)}$\n",
    "$$ \\tilde x^{(i)} = \\sum_{k=1}^p w^{(k)} z_k^{(i)} = W z^{(i)} $$\n",
    "Note that $\\tilde x^{(i)}$ is the closest point in the subspace to $x^{(i)}$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "459c630c",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "## Principal Components Model\n",
    "\n",
    "Principal component analysis (PCA) assumes that \n",
    "* Datapoints $x \\in \\mathbb{R}^{d}$ live close to a low-dimensional subspace $\\mathcal{Z} = \\mathbb{R}^p$ of dimension $p<d$\n",
    "* The subspace $\\mathcal{Z} = \\mathbb{R}^p$ is spanned by a set of orthonormal vectors $w^{(1)}, w^{(2)}, \\ldots, w^{(p)}$\n",
    "* The data $x$ are approximated by a linear combination $\\tilde x$ of the $w^{(k)}$\n",
    "$$ x \\approx \\tilde x = \\sum_{k=1}^p w^{(k)} z_k = W z $$\n",
    "for some $z \\in \\mathcal{X}$ that are the coordinates of $\\tilde x$ in the basis $W$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5127aa37",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "This model enables performing two tasks:\n",
    "* __Encoding__: $z = W^\\top x$, finding the low-dimensional form of input $x$\n",
    "* __Decoding__: $\\tilde x = W  z$, converting a low-dimensional $z$ to a high-dimensional representation $x$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a5aced9",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## PCA Objective: Maximizing Variance\n",
    "\n",
    "We discussed in class that PCA has two objectives, one related to matrix factorization and minimizing the reconsturction error and one alternative objective for learning a PCA model is maximizing variance. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80df352b",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "Below, we can project the data along the blue line or the orange line.\n",
    "\n",
    "The blue line is better because it captures the shape of the data and can be naturally interpreted as \"sepal size\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc0c5c68",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "#import standard libraries\n",
    "from sklearn.datasets import load_iris\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "import plotly.io as pio\n",
    "import seaborn as sns\n",
    "pio.renderers.default = \"iframe\"\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "# Load the Iris dataset\n",
    "iris = load_iris()\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "plt.rcParams['figure.figsize'] = [12, 4]\n",
    "\n",
    "# Visualize the Iris flower dataset\n",
    "setosa_flowers = (iris.target == 0)\n",
    "plt.scatter(iris.data[setosa_flowers,0], iris.data[setosa_flowers,1], alpha=0.5)\n",
    "plt.plot([4.3, 5.8], [2.8, 4.2], '->')\n",
    "plt.plot([5.05, 4.99], [2.85, 4.1])\n",
    "plt.ylabel(\"Sepal width (cm)\")\n",
    "plt.xlabel(\"Sepal length (cm)\")\n",
    "plt.legend(['\"Size\" Dimension', '\"Other\" Dimension'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a349bec",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "How do we automatically identify natural directions of variation in the data? Consider the following dataset (image by [Andrew Ng](http://cs229.stanford.edu/)).\n",
    "\n",
    "<center><img width=45% src=\"img/pca_projection_data.png\"></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c65b529",
   "metadata": {},
   "source": [
    "### Question 0:\n",
    "\n",
    "Which of the following 2 lines can summarize the variance in a better way? A or B? Justify your answer shortly"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74b1dda5",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "A\n",
    "<center><img width=50% src=\"img/pca_projection1.png\"></center>\n",
    "\n",
    "B\n",
    "<center><img width=50% src=\"img/pca_projection2.png\"></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32792511",
   "metadata": {},
   "source": [
    "_Type your answer here, replacing this text._"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "858c5c15",
   "metadata": {},
   "source": [
    "# Part 2: Applying PCA on the Iris dataset\n",
    "\n",
    "To begin, run the following cell to (re)load the dataset into this notebook (if you haven't run the previous part). \n",
    "* `iris_features` will contain a numpy array of 4 attributes for 150 different plants (shape `150 x 4`). \n",
    "* `iris_target` will contain the class of each plant. There are 3 classes of plants in the dataset: Iris-Setosa, Iris-Versicolour, and Iris-Virginica. The class names will be stored in `iris_target_names`.\n",
    "* `iris_feature_names` will be a list of 4 names, one for each attribute in `iris_features`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36cb32ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "#from sklearn.datasets import load_iris  #should be already loaded\n",
    "iris_data = load_iris() # Loading the dataset\n",
    "\n",
    "# Unpacking the data into arrays\n",
    "iris_features = iris_data['data']\n",
    "iris_target = iris_data['target']\n",
    "iris_feature_names = iris_data['feature_names']\n",
    "iris_target_names = iris_data['target_names']\n",
    "\n",
    "# Convert iris_target to string labels instead of int labels currently (0, 1, 2) for the classes\n",
    "iris_target = iris_target_names[iris_target]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0108893b",
   "metadata": {},
   "source": [
    "Let's explore the data by creating a scatter matrix of our iris features. To do this, we'll create 2D scatter plots for every possible pair of our four features. This should result in six total scatter plots in our scatter matrix with the classes labeled in distinct colors for each plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dc1793f",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(14, 10))\n",
    "plt.suptitle(\"Scatter Matrix of Iris Features\")\n",
    "plt.subplots_adjust(wspace=0.3, hspace=0.3)\n",
    "for i in range(1, 4):\n",
    "    for j in range(i):\n",
    "        plt.subplot(3, 3, i+3*j)\n",
    "        sns.scatterplot(x=iris_features[:, i], y=iris_features[:, j], hue=iris_target)\n",
    "        plt.xlabel(iris_feature_names[i])\n",
    "        plt.ylabel(iris_feature_names[j])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "950fe595",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "## Question 1a\n",
    "\n",
    "To apply PCA, we will first need to center and scale the data so that the mean of each feature is 0, and the standard deviation of each feature is 1. \n",
    "\n",
    "Compute the columnwise mean of `iris_features` in the cell below and store it in `iris_mean`, and compute the columnwise standard deviation of `iris_features` and store it in `iris_std`. Each should be a numpy array of 4 means, 1 for each feature. Then, subtract `iris_mean` from `iris_features` and divide by `iris_std`, and finally, save the result in `features`.\n",
    "\n",
    "**Hints:** \n",
    "* Use `np.mean` or `np.average` to compute `iris_mean`, and pay attention to the `axis` argument.\n",
    "* If you are confused about how numpy deals with arithmetic operations between arrays of different shapes, see this note about [broadcasting](https://docs.scipy.org/doc/numpy/user/basics.broadcasting.html) for explanations/examples.\n",
    "\n",
    "<!--\n",
    "BEGIN QUESTION\n",
    "name: q1a\n",
    "-->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a6326db",
   "metadata": {},
   "outputs": [],
   "source": [
    "iris_mean = ...\n",
    "iris_std = ...\n",
    "iris_standardized = ...\n",
    "iris_mean, iris_std"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92eea38c",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "## Question 1b\n",
    "\n",
    "As you may recall from lecture, PCA is a specific application of the singular value decomposition (SVD) for matrices. In the following cell, let's use the [`np.linalg.svd`](https://docs.scipy.org/doc/numpy/reference/generated/numpy.linalg.svd.html) function to compute the SVD of our `features`. Store the left singular vectors, singular values, and right singular vectors in `u`, `s`, and `vt`, respectively. Note that `vt` corresponds to $V^T$. Set the `full_matrices` argument of `np.linalg.svd` to `False`.\n",
    "\n",
    "<!--\n",
    "BEGIN QUESTION\n",
    "name: q1b\n",
    "-->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d57c29d",
   "metadata": {},
   "outputs": [],
   "source": [
    "u, s, vt = ...\n",
    "print(f\"Dimensions of U: {u.shape}\")\n",
    "print(f\"1D List of diagonal elements of Sigma: {s}\")\n",
    "print(f\"Dimensions of V Transpose: {vt.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe7aa76d",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "## Question 1c\n",
    "\n",
    "What can we learn from the singular values in `s`? Formally, we can measure the amount of variance captured by the i'th principal component as:\n",
    "\n",
    "$\\sigma_i^2/N$, where $\\sigma_i$ is the singular value of the i'th principal component and $N$ is the total number of data points.\n",
    "\n",
    "Compute the total variance of our data below by summing the square of each singular value in `s` and dividing the result by the total number of data points. Store the result in the variable `total_variance`.\n",
    "\n",
    "<!--\n",
    "BEGIN QUESTION\n",
    "name: q1c\n",
    "-->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f8071fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "iris_total_variance = ...\n",
    "\n",
    "print(\"iris_total_variance: {:.3f} should approximately equal the sum of the feature variances: {:.3f}\"\n",
    "      .format(iris_total_variance, np.sum(np.var(iris_standardized, axis=0))))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afaf3bba",
   "metadata": {},
   "source": [
    "As you can see, `total_variance` is equal to the sum of the feature variances."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e434caf",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "## Question 2a\n",
    "\n",
    "Let's now use only the first two principal components to see what a 2D version of our iris data looks like.\n",
    "\n",
    "First, construct the 2D version of the iris data by multiplying our `features` array with the first two right singular vectors in `v`. Because the first two right singular vectors are directions for the first two principal components, this will project the iris data down from a 4D subspace to a 2D subspace.\n",
    "\n",
    "**Hints:**\n",
    "* To matrix-multiply two numpy arrays, use `@` or `np.dot`.\n",
    "* Note that the output of `np.linalg.svd` is `vt` and not `v`: the first two right singular vectors in `v` will be the first two columns of `v`, or the first two rows of `vt` (transposed to be column vectors instead of row vectors). \n",
    "* Since we want to obtain a 2D version of our iris dataset, the shape of `iris_2d` should be (150, 2).\n",
    "\n",
    "<!--\n",
    "BEGIN QUESTION\n",
    "name: q2a\n",
    "-->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57378171",
   "metadata": {},
   "outputs": [],
   "source": [
    "features.shape, vt.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5673ca37",
   "metadata": {},
   "outputs": [],
   "source": [
    "iris_2d = ...\n",
    "iris_2d.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c277512",
   "metadata": {},
   "source": [
    "Now, run the cell below to create the scatter plot of our 2D version of the iris data, `iris_2d`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e145229e",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (9, 6))\n",
    "plt.title(\"PC2 vs. PC1 for Iris Data\", fontsize = 18)\n",
    "plt.xlabel(\"Iris PC1\", fontsize = 15)\n",
    "plt.ylabel(\"Iris PC2\", fontsize = 15)\n",
    "sns.scatterplot(x = iris_2d[:, 0], y = iris_2d[:, 1], hue = iris_target);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0974707c",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "## Question 2b\n",
    "\n",
    "What do you observe about the plot above? If you were given a point in the subspace defined by PC1 and PC2, how well would you be able to classify the point as one of the three iris types?\n",
    "\n",
    "<!--\n",
    "BEGIN QUESTION\n",
    "name: q2b\n",
    "-->"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e5f5e67",
   "metadata": {},
   "source": [
    "_Type your answer here, replacing this text._"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76e4e6a4",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "## Question 2c\n",
    "\n",
    "What proportion of the total variance is accounted for when we project the iris data down to two dimensions? Compute this quantity in the cell below by dividing the variance captured by the first two singular values (also known as component scores) in `s` by the `total_variance` you calculated previously. Store the result in `two_dim_variance`.\n",
    "\n",
    "<!--\n",
    "BEGIN QUESTION\n",
    "name: q2c\n",
    "-->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51b8430f",
   "metadata": {},
   "outputs": [],
   "source": [
    "two_dim_variance = ...\n",
    "two_dim_variance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8eaa507",
   "metadata": {},
   "source": [
    "Most of the variance in the data is explained by the two-dimensional projection!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cab272aa",
   "metadata": {},
   "source": [
    "## Question 3\n",
    "\n",
    "As a last step, we will create a [scree plot](https://en.wikipedia.org/wiki/Scree_plot) to visualize the weight of each principal component. In the cell below, create a scree plot by creating a line plot of the component scores (variance captured by each principal component) vs. the principal component number (1st, 2nd, 3rd, or 4th). Your graph should match the image below:\n",
    "\n",
    "***Hint***: You may find `plt.xticks()` helpful when formatting your plot axes.\n",
    "\n",
    "<img src=\"img/scree.png\" width=\"400px\" />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb1a3e73",
   "metadata": {},
   "outputs": [],
   "source": [
    "#your plot goes here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdc4c4bb",
   "metadata": {},
   "source": [
    "<br/>\n",
    "\n",
    "---\n",
    "\n",
    "## [Tutorial] Biplots\n",
    "\n",
    "Finally, we will analyze the **biplot** ([link](https://en.wikipedia.org/wiki/Biplot)) to understand how each feature contributes to the first two principal components. We do this by plotting the **directions**, or rows of $V^T$, which indicate how a feature correlates with each respective principal component. \n",
    "\n",
    "Recall that the columns of $U\\Sigma$ are the principal components of $X$. Because $V^T$ is an orthonormal matrix:\n",
    "\n",
    "\\begin{eqnarray}\n",
    "X &=& U\\Sigma V^T \\\\\n",
    "XV &=& U\\Sigma V^T V = U\\Sigma I\\\\\n",
    "XV &=& U\\Sigma\n",
    "\\end{eqnarray}\n",
    "\n",
    "\n",
    "The direction vector $\\vec{v}_1$ indicates the amount with which to scale each feature vector to construct the first principal component. For example, if we define the principal component as $(U\\Sigma)_1 = \\sigma_1\\vec{u}_1$ and $\\vec{v}_j$ as the $j$-th direction (and therefore the $j$-th row of $V^T$):\n",
    "\n",
    "$$\\begin{bmatrix}\n",
    "| & & | \\\\\n",
    "\\vec{x}_1 & \\cdots & \\vec{x}_d \\\\\n",
    "| & & | \\\\\n",
    "\\end{bmatrix} \\begin{bmatrix}\n",
    "| & | & \\\\\n",
    "\\vec{v}_1 & \\vec{v}_2 & \\cdots \\\\\n",
    "| & | & \\\\\n",
    "\\end{bmatrix} =\n",
    "\\begin{bmatrix}\n",
    "| & \\\\\n",
    "\\sigma_1\\vec{u}_1 & \\cdots  \\\\\n",
    "| & \\\\\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "**Note:** SVD decomposition is not unique. For example,  \n",
    "\\begin{eqnarray}\n",
    "X &=& U\\Sigma V^T \\\\\n",
    "&=& (-U) \\Sigma (-V)^T \\\\\n",
    "&=& \\tilde{U} \\Sigma \\tilde{U}^T \\\\\n",
    "& \\tilde{U} = -U, \n",
    "& \\tilde{U} = -V\n",
    "\\end{eqnarray}\n",
    "\n",
    "Here, $X = \\tilde{U} \\Sigma \\tilde{V}^T$ is another valid SVD decomposition, and $\\tilde{U} \\Sigma$ is another valid principal component of X. The singular vectors $\\tilde{U}$ have the opposite sign than $U$. We set the random seed here to ensure we can reproduce the same decomposition. \n",
    "\n",
    "\n",
    "Run the below cell to generate the biplot for the Iris dataset. Based on the principal components plotted in the above biplot, what can you say about how each feature contributes to PC1 and PC2?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "991ec72f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell to plot a biplot; no further action is needed.\n",
    "import random\n",
    "random.seed(42)\n",
    "cp = sns.color_palette()[1:] # Skip blue\n",
    "\n",
    "plt.figure(figsize = (7, 7))\n",
    "\n",
    "# First plot each datapoint in terms of the first two principal components.\n",
    "sns.scatterplot(x = iris_2d[:, 0], y = iris_2d[:, 1]);\n",
    "\n",
    "# Next, plot the loadings for PC1 and PC2.\n",
    "dir1, dir2 = vt[0,:], vt[1,:]\n",
    "# Just plotting the 2 arrows corresponding to 'sepal_width' and 'petal_length' \n",
    "for i, feature in enumerate(['', 'sepal width', 'petal length', '']):\n",
    "    plt.arrow(0, 0,\n",
    "              dir1[i], dir2[i],\n",
    "              head_width=0.2, head_length=0.2, color=cp[i])\n",
    "    plt.text(dir1[i] * 1+0.1*random.random(), # jitter\n",
    "             dir2[i] * 1+0.1*random.random(), \n",
    "             feature, fontsize=18, color=cp[i],\n",
    "             backgroundcolor=(1,1,1,0.6))\n",
    "\n",
    "plt.title(\"Iris Biplot\")\n",
    "plt.xlabel(\"Iris PC1\")\n",
    "plt.ylabel(\"Iris PC2\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b16b947",
   "metadata": {},
   "source": [
    "## Question 4\n",
    "\n",
    "Based on the principal components plotted in the above biplot, fill in the blanks for each of the following statements:\n",
    "\n",
    "Q4a. `sepal width` looks to be ___________ with PC1.<br/>\n",
    "Q4b. `sepal width` looks to be ___________ with PC2.<br/>\n",
    "Q4c. `petal length` looks to be ___________ with PC1.<br/>\n",
    "Q4d. `petal length` looks to be ___________ with PC2.\n",
    "\n",
    "Note that we have displayed the arrows for all the features in the dataset, though you only need to look at the labeled arrows to answer the question (green - sepal width, red - petal length).\n",
    "\n",
    "You should assign each variable (e.g., `q4a`) to`'A'`, `'B'`, `'C'` corresponding to the below:\n",
    "\n",
    "A. positively correlated<br/>\n",
    "B. negatively correlated <br/>\n",
    "C. uncorrelated\n",
    "\n",
    "In some cases, it may be difficult to draw the line between positively/negatively correlated and uncorrelated."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a66e9e70",
   "metadata": {},
   "source": [
    "*Answers:*\n",
    "\n",
    "q4a = ...\n",
    "<br>\n",
    "q4b = ...\n",
    "<br>\n",
    "q4c = ...\n",
    "<br>\n",
    "q4d = ...\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16176bd3",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Part 3: PCA on 3D Data\n",
    "\n",
    "**In Part 3, our goal is to see visually how PCA is simply the process of rotating the coordinate axes of our data.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "969fed9a",
   "metadata": {},
   "source": [
    "## Question 5\n",
    "\n",
    "To get a better understanding of what PCA is doing to a dataset, let’s imagine applying it to points contained within this surfboard. If you are a surfer, shout aloud in class “I LOVE TO SURF” and you will get a free coffee for the next lecture. Offer is limited to the first three that opt for it.\n",
    "\n",
    "<center>\n",
    "<img src=\"img/surfboard.png\" width=\"250px\"/>\n",
    "</center>\n",
    "\n",
    "The origin is in the center of the board, and each point within the board has three attributes: how far (in cm) along the board’s length, width, and thickness the point is from the center. These three dimensions determine the spread of the data.\n",
    " \n",
    "If we were to apply PCA to the surfboard, what would the first three principal components (PCs) represent? Feel free to draw and label these dimensions on the image of the surfboard.\n",
    "\n",
    "Additional thought: Which of the 3 PCS should be used to create a 2D  representation of the surfboard? How come? Try to make a sketch of the 2D projection."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00caf720-ace6-4d8d-b52d-9e0b680a3b43",
   "metadata": {},
   "source": [
    "_Type your answer here, replacing this text._"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d5fd124-0a6e-465e-bbd5-4213b2bcffe8",
   "metadata": {},
   "source": [
    "The code below reads in a 3D dataset. We named the `DataFrame` `surfboard` because the data resembles a surfboard when plotted in 3D space. First, let's do a \"thought\" exercise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7fd90e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell to see the plot; no further action is needed.\n",
    "surfboard = pd.read_csv(\"data/data3d.csv\")\n",
    "surfboard.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf8c091b",
   "metadata": {},
   "source": [
    "### [Tutorial] Visualize the Data\n",
    "\n",
    "The cell below will allow you to view the data as a 3D scatterplot. Rotate the data around and zoom in and out using your trackpad or the controls at the top right of the figure.\n",
    "\n",
    "You should see that the data is an ellipsoid that looks roughly like a surfboard or a hashbrown patty ([link](https://www.google.com/search?q=hashbrown+patty&source=lnms&tbm=isch)). It is pretty long in one direction, pretty wide in another, and relatively thin along its third dimension. We can think of these as the \"length\", \"width\", and \"thickness\" of the surfboard data.\n",
    "\n",
    "Observe that the surfboard is not aligned with the x/y/z axes.\n",
    "\n",
    "If you get an error that your browser does not support webgl, you may need to restart your kernel and/or browser."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec95a359",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell to see the plot; no further action is needed.\n",
    "fig = px.scatter_3d(surfboard, \n",
    "                    x='x', y='y', z='z', \n",
    "                    range_x = [-10, 10], range_y = [-10, 10], range_z = [-10, 10], height = 500, width = 600)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08ea26b1",
   "metadata": {},
   "source": [
    "**Visualize the Data (Colorized)**\n",
    "\n",
    "To give the figure a little more visual pop, the following cell does the same plot but assigns a pre-determined color value (that we've arbitrarily chosen) to each point. *These colors do not mean anything important*; they're simply there as a visual aid.\n",
    "\n",
    "You might find using the `colorize_surfboard_data` method useful later in this lab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "146c6b73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell to see the colorized version of the previous cell; no further action is needed.\n",
    "def colorize_surfboard_data(df):\n",
    "    colors = pd.read_csv(\"data/surfboard_colors.csv\", header = None).values\n",
    "    df_copy = df.copy()\n",
    "    df_copy.insert(loc = 3, column = \"color\", value = colors)\n",
    "    return df_copy\n",
    "    \n",
    "fig = px.scatter_3d(colorize_surfboard_data(surfboard), \n",
    "                    x='x', y='y', z='z', \n",
    "                    range_x = [-10, 10], range_y = [-10, 10], range_z = [-10, 10], \n",
    "                    color = \"color\", color_continuous_scale = 'RdBu', height = 500, width = 600)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f39b783",
   "metadata": {},
   "source": [
    "### Question 6a\n",
    "\n",
    "In the previosu part, we standardized the Iris data before performing SVD, i.e., we made features zero-mean and unit-variance. In this part, we'll try just **centering** our data so that each feature is zero-mean and variance is unchanged.\n",
    "\n",
    "Compute the column-wise mean of `surfboard` in the cell below and store the result in `surfboard_mean`. You can make `surfboard_mean` a `NumPy` array or a `Series`, whichever is more convenient. Regardless of your data type, `surfboard_mean` should have 3 means: 1 for each attribute, with the $x$ coordinate first, then $y$, then $z$.\n",
    "\n",
    "Then, subtract `surfboard_mean` from `surfboard`, and save the result in `surfboard_centered`. The order of the columns in `surfboard_centered` should be $x$, then $y$, then $z$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9a0b619",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Compute the column-wise mean\n",
    "surfboard_mean = ...\n",
    "\n",
    "# Center the data by subtracting the mean\n",
    "surfboard_centered = ...\n",
    "\n",
    "# Display the first few rows of the centered data\n",
    "surfboard_centered.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3963c2c",
   "metadata": {},
   "source": [
    "### Question 6b\n",
    "\n",
    "In the following cell, compute the SVD of `surfboard_centered` as $U\\Sigma V^T$, and store the left singular vectors $U$, singular values / diagonal elements of $\\Sigma$, and (transposed) right singular vectors $V^T$ in `u2`, `s2`, and `vt2`, respectively.\n",
    "\n",
    "Your code should be very similar to Question 1b."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2a89238",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Perform SVD\n",
    "u2, s2, vt2 = ...\n",
    "\n",
    "# Display results\n",
    "u2, s2, vt2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40c8d4a0",
   "metadata": {},
   "source": [
    "### Question 6c: Total Variance\n",
    "\n",
    "In Question 1c, we considered standardized features (each with unit variance), whose total variance was simply the count of features. Now, we'll show that the same relationship holds between singular values `s` and the variance of our (unstandardized) data.\n",
    "\n",
    "In the cell below, compute the total variance as the sum of the squares of the singular values $\\sigma_i$ divided by the number of datapoints $n$. Here's that formula again from previous questions:\n",
    "\n",
    "$$\\text{Var}(X) = \\frac{\\sum_{i=1}^p{\\sigma_i^2}}{n} = \\sum_{i=1}^p \\frac{\\sigma_i^2}{n}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f665389",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Number of data points (rows in the dataset)\n",
    "n = ...\n",
    "\n",
    "# Compute total variance\n",
    "total_variance_computed_from_singular_values = ...\n",
    "\n",
    "# Display the result\n",
    "total_variance_computed_from_singular_values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "408605db",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "<br/>\n",
    "\n",
    "Your `total_variance_computed_from_singular_values` result should be very close to the total variance of the original `surfboard` data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4153005c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell to check the result; no further action is needed.\n",
    "np.var(surfboard, axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9004bf8",
   "metadata": {},
   "source": [
    "The total variance of our dataset is given by the sum of these numbers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6df21122",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell to check the result; no further action is needed.\n",
    "total_variance_computed_from_surfboard = sum(np.var(surfboard, axis=0))\n",
    "total_variance_computed_from_surfboard"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ec166f2",
   "metadata": {},
   "source": [
    "Note: The variances are the same for both `surfboard_centered` and `surfboard` (why?), so we show only one to avoid redundancy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de1d1ba7",
   "metadata": {},
   "source": [
    "### Question 6d: Variance Explained by First Principal Component"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adbcbaf7",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "In the cell below, set `variance_explained_by_1st_pc` to the proportion of the total variance explained by the 1st principal component. Your answer should be a number between 0 and 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db680562",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Compute the variance explained by the first principal component\n",
    "variance_explained_by_1st_pc = ...\n",
    "\n",
    "# Display the result\n",
    "variance_explained_by_1st_pc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "031ce22f",
   "metadata": {},
   "source": [
    "We can also create a scree plot that shows the proportion of variance explained by all of our principal components, ordered from most to least. You already constructed a scree plot for the Iris data, so we'll leave the surfboard scree plot for you to do on your own time.\n",
    "\n",
    "Instead, let's try to visualize why PCA is simply a rotation of the coordinate axes (i.e., features) of our data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6f3a0d0",
   "metadata": {},
   "source": [
    "## Question 7: V as a Rotation Matrix\n",
    "\n",
    "In the lecture, we saw that the first column of $XV$ contained the first principal component values for each observation, the second column of $XV$ contained the second principal component values for each observation, and so forth.\n",
    "\n",
    "Let's name this matrix: $P = XV = U\\Sigma$ is sometimes known as the \"principal component matrix\".\n",
    "\n",
    "Compute the $P$ matrix for the surfboard dataset and store it in the variable `surfboard_pcs`.\n",
    "\n",
    "**Hint:** What does $X$ represent here: `surfboard` or `surfboard_centered`? Why?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbe108b4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Compute the principal component matrix\n",
    "surfboard_pcs = ...\n",
    "\n",
    "# Display the first few rows\n",
    "surfboard_pcs[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74940608",
   "metadata": {},
   "source": [
    "### [Tutorial] Visualizing the Principal Component Matrix\n",
    "\n",
    "We can think of $P$ as an output of the PCA procedure. $P$ is a **rotation** of the data such that the data will now appear \"axis aligned\". Specifically, for a 3d dataset, if we plot PC1, PC2, and PC3 along the $x$, $y$, and $z$ axes of our plot, then the greatest amount of variation happens along the $x$-axis, the second greatest amount along the $y$-axis, and the smallest amount along the $z$-axis. \n",
    "\n",
    "To visualize this, run the cell below, showing our data projected onto the principal component space. Compare it with your original figure, and observe that the data is precisely the same—only it is now rotated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edf4394f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Run this cell to see the plot; no further action is needed.\n",
    "# Convert NumPy array to Pandas DataFrame\n",
    "surfboard_pcs = pd.DataFrame(surfboard_pcs, columns=[\"pc1\", \"pc2\", \"pc3\"])\n",
    "\n",
    "# Now the renaming step is unnecessary, but if needed, it would work:\n",
    "# surfboard_pcs = surfboard_pcs.rename(columns={0: \"pc1\", 1: \"pc2\", 2: \"pc3\"})\n",
    "\n",
    "# Plot the transformed data\n",
    "fig = px.scatter_3d(colorize_surfboard_data(surfboard_pcs), \n",
    "                    x='pc1', y='pc2', z='pc3', \n",
    "                    range_x=[-10, 10], range_y=[-10, 10], range_z=[-10, 10], \n",
    "                    color='color', color_continuous_scale='RdBu', height=500, width=600)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d7b8baa",
   "metadata": {},
   "source": [
    "We can also create a 2D scatter plot of our `surfboard` data. Note that the resulting is just the 3D plot as viewed from directly \"overhead\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "799b9655",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Run this cell to see the plot; no further action is needed.\n",
    "fig = plt.figure(figsize=(10, 6))\n",
    "plt.subplot(1, 2, 1)\n",
    "\n",
    "sns.scatterplot(data = colorize_surfboard_data(surfboard_pcs), \n",
    "                x = 'pc1', y = 'pc2', hue = \"color\", palette = \"RdBu\", legend = False)\n",
    "plt.xlim(-10, 10);\n",
    "plt.ylim(-10, 10);\n",
    "plt.title(\"Top-Down View of $P$\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c74ab3e",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Part 4: PCA on a real dataset\n",
    "\n",
    "We are going to use data from different US states related to Murder, Assault, Rape and Urban Population and try do some PCA in order to discover some interesting perspectives about wild US. We will also use `sklearn`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c93d2a0e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#imports to come handy later\n",
    "from sklearn.preprocessing import scale\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Load the data\n",
    "df = pd.read_csv('data/USArrests.csv', index_col=0)\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfd763f8",
   "metadata": {},
   "source": [
    "### Question 8\n",
    "\n",
    "Compute the variance values for the 4 variables. What do you observe? \n",
    "\n",
    "Do we need scaling or not and why? If so include it in the code below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b5ac7f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#your code goes here\n",
    "#compute variance\n",
    "var = ...\n",
    "\n",
    "#scale the data if needed, store result to X\n",
    "X = ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac415a37",
   "metadata": {},
   "source": [
    "_Type your answer here, replacing this text._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6635a38",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Fit the PCA model\n",
    "\n",
    "# transform the data to get the principal components\n",
    "# store the results to the following dataframes\n",
    "\n",
    "#pca result on the data\n",
    "#use pd.Dataframe(..., columns=['PC1', 'PC2', 'PC3', 'PC4'], index=X.index)\n",
    "df_pca = ...\n",
    "\n",
    "\n",
    "#contributions of original variables to PCs\n",
    "#use pd.Dataframe(..., index=df.columns, columns=['V1', 'V2', 'V3', 'V4'])\n",
    "pca_loadings = ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12ff6c0f",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "The plot below will run and give you the biplot. No further action is needed.\n",
    "\n",
    "Pay attention that the variables `df_pca` and `pca_loadings` should be dataframes with the columns as shown in the comments of the code to work properly. Feel free to make modifications as needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12dc719d",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fig , ax1 = plt.subplots(figsize=(9,7))\n",
    "\n",
    "#ax1.set_xlim(-3.5,3.5)\n",
    "#ax1.set_ylim(-3.5,3.5)\n",
    "\n",
    "# Plot Principal Components 1 and 2\n",
    "for i in df_pca.index:\n",
    "    ax1.annotate(i, (df_pca.PC1.loc[i], -df_pca.PC2.loc[i]), ha='center')\n",
    "\n",
    "# Plot reference lines\n",
    "ax1.hlines(0,-3.5,3.5, linestyles='dotted', colors='grey')\n",
    "ax1.vlines(0,-3.5,3.5, linestyles='dotted', colors='grey')\n",
    "\n",
    "ax1.set_xlabel('First Principal Component')\n",
    "ax1.set_ylabel('Second Principal Component')\n",
    "    \n",
    "# Plot Principal Component loading vectors, using a second y-axis.\n",
    "ax2 = ax1.twinx().twiny() \n",
    "\n",
    "ax2.set_ylim(-1,1)\n",
    "ax2.set_xlim(-1,1)\n",
    "ax2.tick_params(axis='y', colors='orange')\n",
    "ax2.set_xlabel('Principal Component loading vectors', color='orange')\n",
    "\n",
    "# Plot labels for vectors. Variable 'a' is a small offset parameter to separate arrow tip and text.\n",
    "a = 1.07  \n",
    "for i in pca_loadings[['V1', 'V2']].index:\n",
    "    ax2.annotate(i, (pca_loadings.V1.loc[i]*a, -pca_loadings.V2.loc[i]*a), color='orange')\n",
    "\n",
    "# Plot vectors\n",
    "ax2.arrow(0,0,pca_loadings.V1[0], -pca_loadings.V2[0])\n",
    "ax2.arrow(0,0,pca_loadings.V1[1], -pca_loadings.V2[1])\n",
    "ax2.arrow(0,0,pca_loadings.V1[2], -pca_loadings.V2[2])\n",
    "ax2.arrow(0,0,pca_loadings.V1[3], -pca_loadings.V2[3]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64ff8f60",
   "metadata": {},
   "outputs": [],
   "source": [
    "#compute the pca explained_variance ratio and the cumulative explained variance ratio\n",
    "#plot the scree plot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "261dfcfc",
   "metadata": {},
   "source": [
    "Finally, comment on how many PCs you would keep on this dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f1b9f8c",
   "metadata": {},
   "source": [
    "_Type your answer here, replacing this text._"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
